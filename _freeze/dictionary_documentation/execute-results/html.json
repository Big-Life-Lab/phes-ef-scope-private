{
  "hash": "1cd42d7595fcd6ea914bb79dc6cfe67d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Terms Dictionary\"\nbibliography: assets/PRQ1.bib\ncsl: https://www.zotero.org/styles/vancouver\nformat:\n  html:\n    theme: default\n    toc: true\n    toc-title: \"Terms\"\n    toc-depth: 2\nparams:\n  dict_path: \"data_raw/2025_07_15_ScopingReview_ReportedConcepts(terms_main).csv\"\n---\n\n<hr class=\"intro-separator\">\n\nA listing of each performance term identified in the PHES-EF scoping review.\n\n\n::: {.cell}\n\n:::\n\n\n\n<hr class=\"term-separator\">\n\n## Ability to continue tracking an event {#ability-to-continue-tracking-an-event}\n\n**ID:** abilityToContinueTrackingAnEvent\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** stability (reliability) • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The ability of the surveillance system to continue tracking an event throughout the event life cycle.\n\n**Description:** An event-based biosurveillance may be quite successful in detecting or forecasting an outbreak of disease, but it may lose its sensitivity as the event progresses (e.g., due to a high background level of indicators following a media announcement).\n\n**Background:** This reported concept was only identified by Corley 2012 in the context of biosurveillance models and systems.\n\n**Measurement:** N/A\n\n<hr class=\"term-separator\">\n\n## Acceptability {#acceptability}\n\n**ID:** acceptability\n\n**See also:** [engagement](#engagement); [collaboration](#collaboration)\n\n**Synonyms:** **Exact:** N/A • **Broad:** participation\n\n<hr class=\"header-separator\">\n\n**Definition:** The willingness of persons and organizations to participate in the surveillance system.\n\n**Description:** Acceptability reflects stakeholders' perceptions and attitudes toward the surveillance system, including their willingness to participate. It encompasses recognition of the system's importance, satisfaction with processes, trust in data management, perceived reporting burden, and alignment with ethical and cultural expectations. High acceptability is supported by clear communication, demonstrated value, privacy protections, and responsiveness to user needs.\n\n**Background:** Acceptability was identified as a foundational concept in the evaluation of public health surveillance, being identified on 34 occassions. In our review, the concept was identified as early as 1988, to reflect &quot;the willingness of individuals and organizations to participate in the surveillance system&quot;. Overall, definitions have remained largely congruent with the one introduced in 1988. However, the concept has evolved through the incorporation of new elements such as trust and engagement.\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Acceptability and engagement {#acceptability-and-engagement}\n\n**ID:** acceptabilityAndEngagement\n\n**See also:** [acceptability](#acceptability); [engagement](#engagement); [collaboration](#collaboration)\n\n**Synonyms:** **Exact:** N/A • **Broad:** participation\n\n<hr class=\"header-separator\">\n\n**Definition:** N/A\n\n**Description:** Acceptability and engagement reflect the willingness to participate in and the degree of involvement with a surveillance system. They are shaped by system design (e.g., simplicity, timeliness), perceived value (e.g., importance, relevance, trust), and social context (e.g., cultural fit, stigma, privacy concerns). High levels of acceptability and engagement improve data quality, sustainability, and stakeholder collaboration but may be limited by political will, resource constraints, and communication barriers.\n\n**Background:** Acceptability as a surveillance system attribute has evolved since its initial conceptualization. The original CDC 1988 definition focused on willingness of individuals and organizations to participate in surveillance activities. The 2001 CDC guidelines maintained this core concept while expanding factors that influence acceptability. Calba et al. (2015) broadened the concept to include not just willingness but &quot;the degree to which each of these users is involved in the surveillance.&quot; Contemporary understanding has further expanded to encompass community engagment, participation, and trust—reflecting a shift from traditional, top-down surveillance approaches to more collaborative models. This evolution acknowledges that modern surveillance systems must be acceptable to a wider range of stakeholders beyond traditional public health departments, including affected communities, healthcare providers, private sector entities, and various levels of government. The concept now implicitly recognizes cultural appropriateness, and ethical considerations as components of acceptability.\n\n**Measurement:** Measuring acceptability often requires qualitative methods to capture user perspectives and motivations.\n\n<hr class=\"term-separator\">\n\n## Accessibility (data) {#accessibility-data}\n\n**ID:** accessibilityData\n\n**See also:** data management and storage; [interoperability](#interoperability)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The ease with which authorized users can retrieve surveillance data and metadata using standardized protocols.\n\n**Description:** [To incoporate FAIR in description]. Accessibility in public health surveillance refers to how readily and consistently surveillance outputs (such as alerts, reports, or predictive model results) and input data (such as raw surveillance data or supporting information) are available to end-users, decision-makers, and planners. This includes availability through standardized and sustainable protocols, user-friendly dissemination mechanisms, and manageable data formats. Accessible systems reduce technical, administrative, logistical, and practical barriers, enabling timely decision-making and effective public health responses.\n\n**Background:** FAIR data concepts, introduced by Yang et al. in 2022, have become increasingly prominent. Definitions of accessibility commonly include providing data “when and how users need them,” although these elements overlap with system availability and communication and dissemination. Accessibility involves both output accessibility (ease of disseminating surveillance results to users) and input accessibility (consistent availability and usability of source data). Effective accessibility also requires sufficient human and technological resources, clear authorization balancing openness with privacy and security, and robust infrastructure for reliable data flow.\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Accuracy {#accuracy}\n\n**ID:** accuracy\n\n**See also:** [precision](#precision); bias\n\n**Synonyms:** **Exact:** validity, data correctness, bias (Peyre 2019, 2022), comparability • **Broad:** data quality?\n\n<hr class=\"header-separator\">\n\n**Definition:** The proportion of data entries that correctly reflect the true value of the data collected.\n\n**Description:** [Comparison to the reference standard, ISO,varying definitions]. Accuracy is influenced by several factors, including the sensitivity and specificity of case definitions, the clairty of surveillance forms (whether hardcopy or electronic), the technical competence and training of staff, and the care exercised in data management. It is positively correlated with several other data quality attributes/terms, including representativeness, precision, completeness, and consistency, and is negatively correlated with bias. System acceptability can influence how accurately data are captured and recorded.\n\n**Background:** <p>Accuracy was identified as an independent concept on eight occassions, distinct from the definitions of &quot;data quality&quot; or &quot;data completeness and correctness&quot;. However, this concept has been defined inconsistently across the literature, with terminology varying by discipline - often referred to as validity, accuracy, or data correctness. The definition of accuracy that we have adopted aligns with the DQO and ISO/IEC 25012 definition.</p>\n<p>The definition aligns with established international standards, including ISO 8000-2:2020, which defines accuracy as the correctness of data relative to real-world conditions. It also corresponds to definitions in the Data Quality Ontology (DQO), which emphasizes accuracy as the correctness and precision with which data represent reality. Accuracy assessment often involves validation procedures, comparisons against reference standards, or independent verification processes.</p>\n\n**Measurement:** The accuracy of a surveillance system can be accessed using historical data or simulations. To assess its effectiveness in detecting outbreaks, key metrcis should include the number of outbreaks detected, false alarms, and outbreaks missed or detected late. These assessments should be integrated into the system's routine workflow and have the ability to be conducted with minimal effort or complexity. Routine reporting should be automated where possible. Essential data include: the number of statistical aberrations detected at a set threshold in a defined period of time (e.g., frequency per month at a given p-value); actions taken in response to signals (e.g., review for data errors, in-depth follow-up analysis of the specific conditions within the syndrome category, manual epidemiologic analysis to characterize a signal, examining data from other systems, and increasing the frequency of reporting from affected sites); resources allocated for follow-upt; public health response (e.g., an alert to clinicians, timely dissemination of information to other health entities, a vaccination campaign, or no further response); documentation of how every recognized outbreak was detected; an assessment of the value of the follow-up effort (e.g., the effort was an appropriate application of public health resources); a detailed description of the agent, host, and environmental conditions of the outbreak; and the number of outbreaks detected only late in their course or in retrospect.\n\n<hr class=\"term-separator\">\n\n## Adaptability {#adaptability}\n\n**ID:** adaptability\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** Flexibility • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** N/A\n\n**Description:** N/A\n\n**Background:** –\n\n**Measurement:** –\n\n<hr class=\"term-separator\">\n\n## Adherence --- suggest change to \"Standards use\" or \"Standards adherence\" {#adherence-suggest-change-to-standards-use-or-standards-adherence}\n\n**ID:** adherence\n\n**See also:** [compliance](#compliance)\n\n**Synonyms:** **Exact:** N/A • **Broad:** standards use\n\n<hr class=\"header-separator\">\n\n**Definition:** The degree to which a surveillance system follows and implements established guidelines, protocols, classification systems, and data exchange standards in its core and supporting functions.\n\n**Description:** Standardization encompasses adherence to recognized guidelines across multiple dimensions of surveillance systems, including methodological protocols, data structures, classification systems, and exchange formats. This attribute covers both technical standards (data formats, interoperability frameworks) and operational standards (case definitions, reporting procedures).  Standards use is closely related to compliance, but differs in focus. While standards use focuses on internal consistency in following best practices and operational protocols, compliance is centred around relevant legislation, regulations and policies, including ethics and security. Beyond compliance, standards use is also tied to other data quality attributes/terms, and is positively correlated with consistency ans technical competence and training.\n\n**Background:** <p>The concept of adherence was identified six times in our scoping review. However, the overarching concept referred to here was only identified by Yang et al., (2022), with elements of this definition being referenced multiple times throughout the literature (e.g., standards use, transparency).</p>\n<p>High adherence is essential for the consistent functioning of the system, as it reduces errors, ensures uniformity, and facilitates the accurate tracking of health-related events or environmental changes. A system with high adherence is more likely to produce accurate and high-quality data, as following established protocols reduces errors and inconsistencies. It also enhances consistency by ensuring uniform application of procedures across time and by different personnel. Furthermore, training and supervision play a critical role in maintaining adherence, as they ensure that individuals are well-equipped to follow protocols correctly and consistently.</p>\n\n**Measurement:** Evaluating adherence involves assessing key factors such as the presence of an auditing process to ensure protocols are consistently followed, and whether data collection methods are standardized to maintain high-quality data. It also requires examining the use of harmonized indicators and metrics across sectors to ensure consistent data interpretation. Additionally, adherence can be evaluated by reviewing the use of standardized data elements (e.g., LOINC for lab tests) and electronic data interchange protocols (e.g., Health Level 7). Regular documentation of system protocols, including surveillance strategies and methodologies, is important. Additionally, verifying the enforcement of mandatory requirements for system implementation and data reporting is critical for ensureing full adherence\n\n<hr class=\"term-separator\">\n\n## Availability (system) {#availability-system}\n\n**ID:** availablity\n\n**See also:** Accessibility (data); N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The ability to be operational when needed (Groseclose 2017; Peyre 2019, Peyre 2022; Muellner 2018; Hoinville 2013; Yang 2022; Albali 2023; Amato 2023; Anonymous 2001; Azofeifa 2018; Babaie 2015; Baker 2006; Buehler 2004, 2008; Calba 2015; Drewe 2012, 2015; Harper 2011; Innes 2022; Lucero-Obusan 2022; Marbus 2020; Meynard 2008; Ng'etich 2021; Sosin 2003; Bordier 2019; Velasova 2015).\n\n**Description:** Availability reflects a system’s readiness to reliably collect, process, and disseminate surveillance data when required. It encompasses the continuous functioning of essential components, including infrastructure, skilled personnel, and communication channels, ensuring timely public health responses.\n\n**Background:** –\n\n**Measurement:** –\n\n<hr class=\"term-separator\">\n\n## Benefit {#benefit}\n\n**ID:** benefit\n\n**See also:** [costs](#costs); [impact](#impact)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The direct and indirect advantages produced by the surveillance system.\n\n**Description:** [Carol's + time horizon component]. The benefit term is influenced by several factors, including the scope of surveillance objectives, the effectiveness of data use in decision-making, and the capacity for early detection and response. It encompasses direct and indirect outcomes such as cost savings, improved public and animal health, enhanced trade, and prevention of losses in productivity, morbidity, and mortality. Benefits are positively correlated with terms such as timeliness, usefulness, and stability, and are shaped by stakeholder engagement and collaboration. The distribution of benefits across sectors also informs the overall value of the surveillance system.\n\n**Background:** Benefit was identified as an independent concept nine times in our scoping review, distinct from related concepts such as costs, impact, and efficiency. The concept was first introduced by Hoinville et al., in 2013 and was consistently included in animal health surveillance system evaluation frameworks thereafter. The definition has remained largely unchanged with the execption of Calba et al., 2015 introducing the idea of assessing whether users are satisfied that their requirements have been met in their definition. Their paper also focused solely on non-monetary benefits. However, despite the alignment of this definition across animal health surveillance evaluation frameworks, benefit was not identified as a standalone concept in the context of public health surveillance.\n\n**Measurement:** The evaluation methods for the benefits of surveillance activities involve identifying and quantifying both direct and indirect benefits. One approach includes contingent valuation methods (CVM) like proportional piling, where stakeholders are asked what they would be willing to pay for improvements such as sanitary information. The evaluation process should list all potential benefits, categorize them, and quantify market benefits where possible. Non-monetary benefits, such as improved public health or consumer confidence, should also be considered using alternative methods like quality-adjusted life years (QALY). Additionally, the distribution of benefits across various stakeholders, such as producers, consumers, and the livestock industry, should be assessed. The relationship between surveillance, intervention, and the mitigation of losses should be explored, as many benefits arise from integrated disease control. Finally, the evaluation should capture indirect benefits like enhanced trade opportunities and the improved capacity to respond to future threats or outbreaks.\n\n<hr class=\"term-separator\">\n\n## Bias (ascertainment performance) {#bias-ascertainment-performance}\n\n**ID:** biasAscertainmentPerformance\n\n**See also:** [accuracy](#accuracy); [precision](#precision); [sensitivity](#sensitivity); [specificity](#specificity); [representativeness](#representativeness); data quality; predictive value positive; false detection rate\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The extent to which a prevalence estimate produced by the surveillance system deviates from the true prevalence value.\n\n**Description:** Case detection bias occurs when systematic measurement errors cause surveillance data to systematically differ from the true occurrence of the health event in the target population. Bias can arise from multiple sources including diagnostic test characteristics, reporting practices, population coverage, sampling methods, case definitions, and temporal factors.\nThe magnitude and impact of bias depends on surveillance objectives. For general population surveillance, bias can distort prevalence estimates and trend analysis, leading to incorrect public health conclusions.\n\n**Background:** <p>Bias was identified as a reported concept six times in our scoping review. The concept was first introduced by Hoinville et al., in 2013 and was consistently included in animal health surveillance system evaluation frameworks thereafter. The definition has remained largely unchanged with the execption of Peyre 2019 &amp; 2022 incorporating the idea of &quot;bias=accuracy&quot; in their definition. Despite its frequent application in evaluating animal health surveillance systems, the term &quot;bias&quot; was not identified in the context of public health surveillance.</p>\n<p>Evaluating bias in surveillance systems is essential for accurately monitoring endemic diseases.</p>\n\n**Measurement:** Evaluating case detection bias (also called ascertainment bias) involves assessing systematic errors that affect a surveillance system's ability to accurately identify and classify cases or events. This includes selection bias (when certain cases or populations are systematically over- or under-represented) and information bias (when data collection methods systematically affect measurement accuracy). Bias represents the consequence of measurement errors that can be characterized through surveillance performance measures such as sensitivity, specificity, and representativeness. Evaluating bias involves assessing potential sources, such as diagnostic sensitivity and specificity, underreporting in passive surveillance, and how the sample population is chosen (e.g., animals from abattoirs might not represent the overall population). Additionally, geographic, demographic, or ecological factors can contribute to bias. To quantify and correct for bias, methods like comparison across multiple data sources, simulation models, statistical methods, or capture-recapture analyses can be used. Assessment of bias (systematic error) should address the following: * Selection bias: systematic differences between sample and target populations * Performance bias: systematic differences between groups in care provided or in exposure to factors other than the interventions of interest * Detection bias: systematic differences in how the outcome is determined (eg, death scene investigation protocols) * Attrition bias: systematic loss to follow up * Reporting bias: systematic differences in how people report symptoms or ideation * Other: biases related to a particular data source\n\n<hr class=\"term-separator\">\n\n## Case definition {#case-definition}\n\n**ID:** caseDefinition\n\n**See also:** bias (case detection)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The standard set of criteria used to identify and classify cases or events for consistent reporting and analyses within a surveillance system.\n\n**Description:** The case definition affects surveillance system performance by determining how accurately and consistently cases are identified and classified. A well-designed case definition supports accurate and consistent detection, improving the quality of the data collected. It directly influences terms like sensitivity, specificity, representativeness, and coverage, and plays a role in determining the system’s cost, acceptability, and flexibility. Clear defined definition may also improve consistency across sites and over time.\n\n**Background:** The concept of case definition was identifed five times throughout our scoping review.\n\n**Measurement:** Qualitative assessment methods have been used to evaluate this concept, such as the use of the EVARISK tool.\n\n<hr class=\"term-separator\">\n\n## Collaboration {#collaboration}\n\n**ID:** collaboration\n\n**See also:** acceptability and engagement; [governance](#governance)\n\n**Synonyms:** **Exact:** partnerships • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The active engagement and coordination among diverse partners and end-users to facilitate exchange of data, information, and knowledge, sharing of capacities, and collaborative implementation of surveillance activities.\n\n**Description:** Collaboration in surveillance systems relies on clear roles, strong legal frameworks, and established inter-sectoral mechanisms. It occurs in four main areas: governance across sectors (health, animal, environment), decision-making at all levels (local to international), interdisciplinary efforts (biosciences, social sciences, engineering), and public-private partnerships (e.g., veterinary companies in antimicrobial resistance). Effective data exchange and a shared databases enhance collaboration. [still working on this one somewhat since multiple terms are being folded into this].\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Communication and dissemination {#communication-and-dissemination}\n\n**ID:** communicationAndDissemination\n\n**See also:** [collaboration](#collaboration)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The ability of the system to deliver information and data in a clear and distinct manner to relevant stakeholders inside and outside of the surveillance system.\n\n**Description:** Effective communication and dissemination in surveillance systems involve sharing clear, relevant, and timely information with all stakeholders, actors, and end-users (both internally and externally). Surveillance data outputs must meet the needs of diverse users, including data providers, analysts, decision-makers, and the public. Outputs should be accessible, frequent, and based on up-to-date, high-quality data with clear explanations of limitations and biases. Communication and disseminationis is also closely linked to acceptability, timeliness, impact, and collaboration.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Compatability {#compatability}\n\n**ID:** compatibility\n\n**See also:** Coherence; [interoperability](#interoperability)\n\n**Synonyms:** **Exact:** N/A • **Broad:** Integration\n\n<hr class=\"header-separator\">\n\n**Definition:** Compatibility with and ability to integrate data from other sources and surveillance components, e.g., one health surveillance (part of data collection and data management) (Peyre 2019, Peyre 2022).\n\n**Description:** N/A\n\n**Background:** –\n\n**Measurement:** –\n\n<hr class=\"term-separator\">\n\n## Compliance {#compliance}\n\n**ID:** compliance\n\n**See also:** [adherence](#adherence-suggest-change-to-standards-use-or-standards-adherence)\n\n**Synonyms:** **Exact:** N/A • **Broad:** legislative support\n\n<hr class=\"header-separator\">\n\n**Definition:** The degree to which the surveillance system complies with all relevant legislation, regulations and policies, including ethics and confidentiality requirements.\n\n**Description:** Compliance refers to meeting required legal or regulatory requirements, often with external oversight. It is closely related to adherence, but differs in focus. While compliance focuses on relevant legislation, regulations and policies, including ethics and security, adherence is centred around internal consistency in following best practices and operational protocols. Beyond adherence, compliance is also tied to other data quality attributes/terms, and is positively correlated with consistency ans technical competence and training.\n\n**Background:** The concept of compliance was identified twice in our scoping review. However, the overarching concept referred to here was only identified by Yang et al., (2022), with components of this definition being referenced multiple times throughout the literature (e.g., legislative support, security, ethical considerations).\n\n**Measurement:** To evaluate compliance, system security policies and practices should be reviewed to ensure that security levels and procedures for surveillance system data or system access are defined and enforced. Data use and release policies and protocol should be available for review, and access to the surveillance system software applications should be controlled. [Need to add information about legislative support/policies and ethics].\n\n<hr class=\"term-separator\">\n\n## Consistency {#consistency}\n\n**ID:** consistency\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** repeatability, precision\n\n<hr class=\"header-separator\">\n\n**Definition:** Data have the same meanings (e.g. case definition, diagnosis standards) to allow for consistent interpretation. This includes internal consistency (within a dataset) and external consistency (across different data sources), as well as consistency over time.\n\n**Description:** Consistency refers to the uniformity and reliability of data across time and sources within the surveillance system. It ensures that data remain unchanged when replicated, transferred, or integrated between partners. Consistent data collection methods, coding systems, case definitions, and documentation of transformations are essential for comparability and trend analysis.\n\n**Background:** Consistency was identified five times in our scoping review.\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Cost-benefit {#cost-benefit}\n\n**ID:** costBenefit\n\n**See also:** cost minimization; [costs](#costs); cost-effectiveness; cost-utility; [benefit](#benefit)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** An analysis where all costs and outcomes are expressed in monetary terms, calculating the net gain or loss by measuring both the costs of an intervention and the monetary value of its outcomes.\n\n**Description:** Cost-benefit analysis quantifies both the costs and outcomes of a surveillance system in monetary terms to assess its net economic value. This allows comparison of investments against financial benefits such as reduced disease burden, healthcare savings, or productivity gains. It supports decision-making by highlighting the economic return of surveillance activities relative to their expense. Accurate valuation of outcomes and comprehensive cost accounting—including direct, indirect, fixed, and variable costs—are essential. This term complements cost-effectiveness and cost-minimization analyses by providing a broader economic perspective on surveillance system value.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Cost-effectiveness {#cost-effectiveness}\n\n**ID:** costEffectiveness\n\n**See also:** cost minimization; cost-benefit; [costs](#costs); cost-utility; [benefit](#benefit)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Relationship between the expected outcomes and the costs of surveillance to achieve these outcomes. Surveillance system costs include direct costs (personnel and material resources), indirect costs (resulting from preparedness and response to surveillance findings), and prevention benefits or costs from a societal perspective (e.g., effects of the information generated on decision making and population health). Refers to the relative expenditures (costs) and outcomes (effects) of different surveillance strategies, determining whether the improved health outcomes or surveillance capabilities justify the associated costs to help inform efficient resource allocation decisions (kirsh 2008 adpated for surveillance).\n\n**Description:** The costs of obtaining surveillance information needs to be balanced against the benefits derived. Assessment of surveillance resources typically focuses on direct costs. Because of the complexity of surveillance and response processes, it is usually difficult to define indirect costs. For some infectious disease surveillance systems, investigators have modeled the expected future costs of strategies for continued vaccination, surveillance, and other public health activities. Efforts to improve sensitivity, positive predictive value, representativeness, timeliness, and stability can increase the cost of a surveillance system.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Cost minimization {#cost-minimization}\n\n**ID:** costMinimization\n\n**See also:** [costs](#costs); cost-benefit; cost-effectiveness; cost-utility; [benefit](#benefit)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The comparison of costs between alternatives with identical outcomes, used to determine the most economical option when no additional benefits are present (often applied to demonstrate the dominance of one strategy over another).\n\n**Description:** Cost-minimization compares the expenses of alternative surveillance strategies that achieve identical outcomes to identify the most economical option. It helps prioritize resource allocation by demonstrating which approach delivers the same public health impact at the lowest cost. This term is closely linked to cost-effectiveness and efficiency, but unlike those, it assumes equivalent benefits across options. Accurate cost data—including fixed and variable costs—are essential. Considering operational, personnel, and infrastructure expenses ensures a comprehensive evaluation, supporting sustainable and affordable surveillance system design without compromising performance.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Cost-utility {#cost-utility}\n\n**ID:** costUtility\n\n**See also:** cost minimization; cost-benefit; cost-effectiveness; [costs](#costs); [benefit](#benefit)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** An evaluation that compares the system's interventions or strategies by measuring outcomes in terms of utility gained, using a ratio scale to assess the value of different health states or surveillance outcomes, allowing for comparison across various surveillance approaches and their impact on public health.\n\n**Description:** Cost-utility analysis compares surveillance strategies by measuring outcomes in terms of utility, often using quality-adjusted life years (QALYs) or disability-adjusted life years (DALYs). This allows valuation of health benefits considering both quantity and quality of life gained from interventions. It facilitates comparison across diverse surveillance approaches with different health impacts, integrating effectiveness and economic costs into a single metric. Accurate utility measurement requires reliable health state valuation and comprehensive cost data. This term complements cost-benefit and cost-effectiveness analyses by focusing on health-related quality of life, supporting informed resource allocation in public health surveillance.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Costs {#costs}\n\n**ID:** costs\n\n**See also:** cost minimization; cost-benefit; cost-effectiveness; cost-utility; [benefit](#benefit)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** All expenses associated with the surveillance system, including direct costs (e.g., equipment, personnel) and indirect costs (e.g., training, data management infrastructure).\n\n**Description:** Cost is a key consideration in evaluating surveillance systems, reflecting their value and efficiency. Costs are both direct (e.g., personnel, data collection, laboratory services, infrastructure) and indirect (e.g., follow-up efforts for false alarms, impact on agency credibility). New or improved systems incur start-up (e.g., software, equipment) and ongoing operational costs (e.g., staff, maintenance). Economic analyses should balance these costs with the public health benefits, such as early outbreak detection and disease prevention, to assess overall value. Direct and indirect costs are usually collected and reported separately.\n\n**Background:** TBD\n\n**Measurement:** To evaluate costs, all elements of the surveillance system must be considered, including planning, sample collection, analysis, reporting, and dissemination. It's important to distinguish between fixed costs (e.g., permanent staff salaries) and variable costs (e.g., sample collection or reagents). Cost-sharing among stakeholders—such as producers, consumers, and public funds—should also be considered, as these costs can vary across the system.\n\n<hr class=\"term-separator\">\n\n## Coverage {#coverage}\n\n**ID:** coverage\n\n**See also:** [representativeness](#representativeness)\n\n**Synonyms:** **Exact:** N/A • **Broad:** representativeness?\n\n<hr class=\"header-separator\">\n\n**Definition:** Proportion of the population of interest (target population) or proportion of areas of interest (e.g. specific habitats or high- risk sites) that is included in the surveillance activity.\n\n**Description:** Coverage indicates the extent to which the target population or geographic areas are included in surveillance. High coverage enhances representativeness, sensitivity, and early detection—especially for emerging threats. It is related to sampling strategy, reporting completeness, and access to high-risk populations. Mathematically, coverage is the proportion the same represents divided by the total target population size.\n\n**Background:** Coverage was identified as a reported concept six times in our scoping review. The concept was first introduced by Hoinville et al., in 2013 and has since been consistently included in animal health surveillance system evaluation frameworks. The definition has remained highly consistent within animal health surveillance, aligning with the concept of &quot;population coverage&quot; in public health surveillance.\n\n**Measurement:** Coverage can be assessed by the proportion of respondents (survey-based) or cases (hospital- or facility-based) included in the surveillance system. Key measurements include population undercoverage, which occurs when parts of the target population are missed, and population overcoverage, where elements outside the target population are included. A demographic analysis can provide benchmarks for assessing completeness of coverage in the existing surveillance data and document changes in coverage from previous periods. Additionally, systems like farmer-based clinical surveillance, syndromic surveillance, or periodic sampling surveys benefit from regular evaluations to identify gaps or misrepresentation in coverage. By reviewing the geographic coverage and the number of areas of interest, it is easier to determine if the surveillance system adequately represents the target population. Furthermore, temporal coverage can be assess by determining the conditional probability that any given unit in the population will be examined or tested within the specified time frame. For example, if the target time frame is 7 days, but testing occurs every 4 weeks, the temporal coverage would be 25%. An assessment of coverage could include: * Characterisation and qualitative comparison of the sampled and target populations. Alternatively, comparison of sampled areas or habitats versus areas or habitats of interest. * Where sufficient data on the target population/areas or habitats of interest exists, simple calculations of the proportional coverage can be made (e.g. 75% of the national herd and 45% of cattle holdings are sampled annually or 30% of the marine ports). * Where sufficient information on the background population, or high-risk areas or habitats respectively, is lacking, more sophisticated sampling designs might be employed (e.g. capture-recapture analysis or drop camera surveillance). * Considering whether the target population, or area of interest, has been adequately defined (i.e. whether the exclusion of certain animals or holdings or sites is merited). * The unit of interest - the level at which coverage is measured - is often the unit of interest of surveillance (e.g. animal, holding, high-risk site or specific marine habitat). If insufficient data exist, alternative perspectives might be desired. Coverage might then be assessed at other aggregate levels (e.g. geographical areas) or relevant intermediate steps in the surveillance pathway (e.g. the proportion of veterinary practices submitting diagnostic samples). * In certain contexts it may be worth establishing a timeframe of reference (e.g. annual coverage). The choice of timeframe should reflect the epidemiology of the disease or life history of a risk organism.\n\n<hr class=\"term-separator\">\n\n## Credibility (collaboration) {#credibility-collaboration}\n\n**ID:** credibilityCollaboration\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** N/A\n\n**Description:** TBD\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Credibility (data) {#credibility-data}\n\n**ID:** credibilityData\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** N/A\n\n**Description:** TBD\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Data accessibility {#data-accessibility}\n\n**ID:** dataAccessibility\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** Accessibility (data) • **Broad:** –\n\n<hr class=\"header-separator\">\n\n**Definition:** –\n\n**Description:** N/A\n\n**Background:** –\n\n**Measurement:** –\n\n<hr class=\"term-separator\">\n\n## Data analysis {#data-analysis}\n\n**ID:** dataAnalysis\n\n**See also:** data collection; data management and storage\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The use of appropriate methods for the analysis and interpretation of data.\n\n**Description:** <em>&lt;+ Data analysis applies scientific methods to process surveillance data to generate insights such as detecting unusual patterns (i.e. outbreaks, signal levels beyond expected). Methods range from basic descriptive statistics and trend monitoring to more sophisticated approaches like time series, spatial analysis, and automated aberration detection algorithms. +&gt;</em>  <del>assesses how effectively surveillance data are processed and interpreted to generate insights.</del> High-performing systems apply appropriate, context-specific analytical methods and ensure findings are actionable.\n\n**Background:** Data analysis was cited as a concept in surveillance system evaluation nine times in the literature, spanning public health, animal health, and One Health surveillance system evaluation frameworks. The definition remains largely consistent, <em>&lt;+ relfecting uniformity across disciplines, with refinment over time, to emphasize the use of contextual information (e.g., prior likelihood of disease) in risk-based analysis and the need for analysis/interpretation to be performed timely to support decision-making and action. +&gt;</em>\n\n**Measurement:** The evaluation of the data analysis attribute/term focuses on assessing the methods used to analyze surveillance data. It includes identifying the techniques applied, such as basic statistics, trend analysis, or more advanced methods like time series and spatial analyses. The evaluation checks if the limitations of the data have been properly considered and addressed in the analysis. Additionally, it evaluates whether the data is being fully utilized or if there are opportunities for further analysis. Reviewing past user needs and whether the analysis methods have met these demands is also important to ensure the analysis is delivering valuable, actionable insights. An evaluation of data analysis should include: The identification of the analysis methods applied to surveillance data: * No analysis * Basic descriptive statistics * Examination of trends * More sophisticated statistical approaches (e.g. time series analyses, spatial analyses); An assessment of whether the limitations of data have been understood and accounted for in statistical analyses?; An indication as to whether the body of data available is being fully exploited or could further use of data be made? It may help to review requirements for information made by users of the surveillance data in the past, to determine whether their needs were met by the methods applied.\n\n<hr class=\"term-separator\">\n\n## Data collection {#data-collection}\n\n**ID:** dataCollection\n\n**See also:** data analysis; data management and storage\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The use of appropriate data sources and collection methods, including a clear a case definition and a data collection protocol.\n\n**Description:** Effective data collection relies on clearly defined and complete case definitions or risk organism descriptions, including case signalment, clinical and pathological signs, and epidemiological information, and, where relevant, laboratory or taxonomic identification. Diagnostic methods must align with the case definition and be assessed for sensitivity and specificity. A written protocol should guide data and sample collection, with mechanisms to ensure adherence. Data collection methods should be clearly documented, and collection methods should minimize redundancy, address data gaps, and employ appropriate sampling strategies (e.g., risk-based or pooled).\n\n**Background:** Data collection is a fundamental surveillance activity consistently recognized in evaluation frameworks across public, animal, and One Health systems (five instances within the literature review). Data collection was explicitly mentioned as a concept in the evaluation of surveillance systems in five instances within the literature, including a clear definition and integration into the animal health surveillance field. It is commonly cited as the most costly and difficult component of a surveillance system to implement (Chaudron, 2010; Peyre et al. 2022).\n\n**Measurement:** Questions to consider when assessing data and information collection include: * Is there (if applicable) a written case definition/organism description for this surveillance system that is clearly defined and complete, with specified inclusion and exclusion criteria? If so Does the case definition/organism description include relevant details of the case signalment, clinical and pathological signs and epidemiological information as appropriate? Does the case definition include laboratory diagnosis? Alternatively does the organism description include taxonomic ID? Are the chosen diagnostic/taxonomic methods appropriate to the case definition/organism description, including in terms of samples being collected? Are syndromes used and - if yes - defined in an appropriate way? In those sectors where symptomatic surveillance is * Is there (if applicable) a written case definition/organism description for this surveillance system that is clearly defined and complete, with specified inclusion and exclusion criteria? If so undertaken, are symptoms clearly defined for when samples need to be taken? * Is there a written sample and/or data collection protocol and are there appropriate assurance mechanisms to ensure the protocols are followed? * Have the sensitivity and specificity of the tests used been assessed (where relevant)? * Are there data collected that are not used in analysis, interpretation or surveillance management (redundancy)? * Are there information needs for which data are not currently collected and feasibly could be? * Are appropriate sampling strategies used, including the use of risk-based approaches and pooled sampling? This could include risk-based requirement calculations or risk- based sampling. The basis of the risks used in the design of the risk-based sampling strategy should be reviewed. Data collection methods should be clearly documented. It may help to review demands for information made by users of the surveillance data in the past, to determine whether their needs were met by the data available.\n\n<hr class=\"term-separator\">\n\n## Data completeness {#data-completeness}\n\n**ID:** dataCompleteness\n\n**See also:** [coverage](#coverage); [accuracy](#accuracy)\n\n**Synonyms:** **Exact:** data completeness and correctness (completeness) • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The proportion of data that were intended to be collected that actually was collected.\n\n**Description:** Completeness refers to the extent to which all required data are captured, reported, and available for analysis. A system with high completeness allows for more reliable decision-making and enhances the system's usefulness. Incomplete data may result from issues with data collection, management, or communication, undermining the system's effectiveness. Regular submission from all contributors and strong data management practices help improve completeness. Ensuring completeness strengthens data quality.\n\n**Background:** Completeness was identified as an independent concept on 12 occassions, distinct from the definitions of &quot;data quality&quot; or &quot;data completeness and correctness&quot;. The concept was first introduced by Thacker et al., in 1988 (A method for evaluating systems of epidemiological surveillance), however it was not indentified as an evaluation concept until 1993 by Cutts et al. The definition of completeness has remained largely unchanged over the years, maintaining consistency across disciplines. It is primarily regarded as a key aspect of data quality and continues to be one of the most frequently used concepts for evaluating surveillance systems.\n\n**Measurement:** An evaluation of completeness should involve assessing the proportion of cases that were identified and subsequently reported to the system, which can be done through register/database reviews. Additionally, examining the percentage of “unknown” or “blank” responses to items on surveillance forms offers a straightforward and effective measure. The evaluation should also address challenges faced in both manual and automated data management. This includes identifying issues such as coding errors or data loss in manual systems, and programming errors or inappropriate data filtering in automated systems. These evaluations help identify gaps in data reporting and ensure that the surveillance system captures all relevant cases accurately.\n\n<hr class=\"term-separator\">\n\n## Data correctness {#data-correctness}\n\n**ID:** dataCorrectness\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** accuracy (correctness), completeness, data quality • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The proportion of data entries that correctly reflect the true value of the data collected.\n\n**Description:** N/A - data completeness and correctness is an exact synonym for data quality.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Data management {#data-management}\n\n**ID:** dataManagement\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** – • **Broad:** –\n\n<hr class=\"header-separator\">\n\n**Definition:** –\n\n**Description:** –\n\n**Background:** –\n\n**Measurement:** –\n\n<hr class=\"term-separator\">\n\n## Data management and storage {#data-management-and-storage}\n\n**ID:** dataManagementAndStorage\n\n**See also:** historical data\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Appropriate use and documentation of data management systems for processing information, including data processing protocols, and effective use of data verification procedures and of data storage and back-up procedures.\n\n**Description:** <p>Appropriate use and documentation of data management systems for processing information, including data processing protocols and effective use of data verification procedures, data storage and back-up protocols. Measures taken to assure authorised computer system access and to maintain confidentiality where needed. Is there a dedicated custodian? Data management is a broad area concerning the collation, storage and maintenance of data, including but not limited to matters of data quality, accessibility, usefulness and security. Assessing this attribute will require an intimate understanding of the data storage and management systems employed by the surveillance activity.</p>\n<p>Data management is a broad area concerning the collation, storage and maintenance of data, including but not limited to matters of data quality, accessibility, usefulness and security. Assessing this attribute will require an intimate understanding of the data systems employed by the surveillance activity.</p>\n<p>Data management and storage involve the systematic handling, processing, and safeguarding of data throughout its lifecycle. This includes ensuring data quality, accessibility, and security. Effective data management requires robust protocols for processing, verification, storage, and backup, as well as measures to control access and maintain confidentiality where necessary. An essential consideration is the appointment of a dedicated custodian responsible for overseeing these activities.</p>\n\n**Background:** TBD\n\n**Measurement:** Evaluating data management and storage involves reviewing the database structure to verify that data fields are well-defined, consistent, and correctly normalized, ensuring efficient storage and retrieval. Evaluators should check that primary keys uniquely identify records and that validation constraints and cross-consistency checks are in place to prevent invalid data input. The documentation supporting the database, such as data dictionaries and entity relationship diagrams, should be reviewed to confirm that it facilitates clear interpretation and understanding of the data. Additionally, the evaluation should assess the existence and sufficiency of data management protocols, including data quality standards (e.g., ISO9000, Good Clinical Practice). This involves reviewing records management policies, retention protocols, and periodic quality control checks. Data security and access control protocols must also be evaluated, ensuring only authorized personnel can access sensitive data.Furthermore, back-up and storage procedures should be tested to confirm data is securely stored and can be recovered if needed. Interviews with data custodians or managers may provide insights into the system's functionality and reliability. Overall, the evaluation will require a comprehensive review of the system's design, documentation, and operational practices to ensure proper data handling and storage. An assessment of this attribute should include: 1.Consideration of whether the database structure has been correctly designed: *Has each field of data been tightly defined to ensure correctness, conciseness and consistency across records? *Have primary keys, uniquely identifying each record, been assigned? *Has the database been normalised, to ensure data is stored in the most parsimonious, transparent and useable way? *Have validation constraints, preventing the input of invalid data, and internal cross-consistency checks been applied? *Is the data stored in a way that allows the required interrogation and analysis?; 2.Consideration of whether documentation of the data is sufficient to facilitate interpretation and understanding of the data: *Is there a document providing a summary overview of the data and collection methods and explaining any idiosyncrasies relevant to the analysis and interpretation of the data? *Is there a data dictionary that clearly defines each field? *Is there an entity relationship diagram that explains how the data relate?; 3.Consideration of whether there are adequate and documented protocols for managing data quality and security: *Is the data management system covered by a data quality standard (e.g. ISO9000, Good Clinical Practice or Good Laboratory Practice)? * Are periodic data quality control checks implemented? *Are records management issues clearly defined, including policy on the retention of data? * Are data back-up and storage protocols in place?\n\n<hr class=\"term-separator\">\n\n## Data quality {#data-quality}\n\n**ID:** dataQuality\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** data completeness and correctness • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The completeness and validity of the data recorded in the public health surveillance system.\n\n**Description:** <em>&lt;+ Data quality  measures the inherent correctness and sufficiency of the data gathered by the surveillance system. +&gt;</em> <del>Data quality reflects the completeness and validity of recorded data and is essential for accurate surveillance outputs.</del> <em>&lt;+ Poor data quality can undermine reliability, introduce bias that limits representativeness, and make the system less acceptable to participants. Conversely, high-quality data are essential for accurate analysis and improving the system's overall usefulness for decision-making and public health action. +&gt;</em>  <del>High-quality data improve system acceptability and representativeness, while</del> Poor-quality data—often due to manual entry errors in paper-based systems—can undermine reliability and may indicate problems in data collection or data management and storage. Electronic systems can enhance data quality by improving accuracy, timeliness, and accessibility. Data quality is linked to terms such as timeliness, acceptability, representativeness, and usefulness, and directly affects the reliability and usability of system outputs for decision-making and public health response.\n\n**Background:** <p>Data quality has historically been defined by the core attributes of completeness and validity. It is routinely recognized as essential across public health, animal health, and emerging One Health surveillance frameworks.  In the context of animal health, the term &quot;data completeness and correctness&quot; is used as an exact synonym for data quality.</p>\n<p>In more recent years, data quality includes the concept of fit-for-purpose, with sufficeint quality to meet the need of the data's intended purpose.  As surveillance systems evolve, particularly through the use of informatics and electronic systems, data quality is enhanced and expanded. Electronic systems can improve data quality by facilitating accuracy, timeliness, accessibility, and reducing the potential for manual entry errors. As data systems evolve, data quality has been enhanced and expanded with the use of robust data management systems and automated data flow processes. The concept is highly interrelated with other performance metrics; for instance, poor quality data reduces system acceptability and representativeness.</p>\n\n**Measurement:** Can be measured as the proportion of data intended to be collected that was actually collected (completeness) and the proportion of data entries that correctly reflect the true value of the data collected (validity/accuracy). Includes proportion of unknown, invalid, and missing values for reported data elements. Validity may be estimated by the proportion of errors in surveillance system data compared to analogous data from one of the system's data sources. Measures for determining data quality include percentages of &quot;unknown,&quot; invalid, and missing responses to items on data collection forms. In addition, data quality can be measured by applying edits for consistency in the data. However, a full assessment may require a special study. The acceptability and representativeness of a public health surveillance system are related to data quality. With data of high quality, the system can be accepted by those who participate in it. In addition, the system can accurately represent the health-related event under surveillance. Paper-based systems, such as the E-Book registry, are more prone to human error, and can result in illegible, missing, and incomplete entries. An electronic system could increase data quality. Moreover, conversion to an electronic system would improve the timeliness, accessibility, and usability of captured data, as well as allow greater flexibility in responding to health events in a community. Many data-quality definitions depend on other system performance attributes (eg, timeliness, usefulness, acceptability) (21). Because of reliance on multiple data sources, data quality must be assessed in different ways. For surveillance relying on surveys, concepts of reliability, validity, and comparison with alternative data sources are important. For example, considerations of possible data-quality concerns arise with use of mortality data, particularly underreporting of suicide. It should be assessed using measures of completeness, consistency, and comparison with alternative sources to ensure reliability across diverse data types and collection methods.\n\n<hr class=\"term-separator\">\n\n## Detection mode {#detection-mode}\n\n**ID:** detectionMode\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The operational capability of the surveillance system to identify or forecast an event (e.g., event detection, establish baseline, or event detection expected with no baseline).\n\n**Description:** For example, are events detected that deviate from the established baseline in frequency and/or magnitude? It should be noted that the baseline fluctuates and that any excess or anomalous occurrence should be statistically characterized and should include some measure of abnormality or confidence\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Effectiveness {#effectiveness}\n\n**ID:** effectiveness\n\n**See also:** [usefulness](#usefulness)\n\n**Synonyms:** **Exact:** efficacy • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The extent to which a surveillance system is able to achieve its intended objectives.\n\n**Description:** Effectiveness reflects how well the surveillance system achieves its intended objectives and meets the needs of stakeholders. It considers factors such as timeliness, sensitivity, specificity, predictive value, and robustness. Systems should use collaborative approaches to improve effectiveness across data collection, processing, and dissemination. Indicators may include task completion, timeliness, and measurable public health impact. This term is closely related to both system usefulness and efficiency.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Efficiency {#efficiency}\n\n**ID:** efficiency\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** cost-effectiveness\n\n<hr class=\"header-separator\">\n\n**Definition:** The link between the resources implemented and the results obtained.\n\n**Description:** An efficient system will accomplish a job with minimum expenditure of time, human effort and cost. Conducting surveillance incurs costs such as salaries, consumables, and travel. These costs can be compared against the outputs from surveillance such as reports, disease or organism detections and notifications, or other signals. A surveillance system can be considered efficient if there is an optimal balance between economic investments and its outputs, the latter achieving the desired quality attributes (e.g. precision, timeliness). Risk-based surveillance can - where appropriate in terms of the surveillance objective - provide efficiency gains in surveillance systems.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Engagement {#engagement}\n\n**ID:** engagement\n\n**See also:** [acceptability](#acceptability); [collaboration](#collaboration)\n\n**Synonyms:** **Exact:** participation • **Broad:** –\n\n<hr class=\"header-separator\">\n\n**Definition:** The active involvement of stakeholders, including partners and communities, in the design, operation, interpretation, and use of the surveillance system.\n\n**Description:** Engagement reflects the depth and quality of participation in surveillance activities. It encompasses contributions to data collection, system design, analysis interpretation, and application of surveillance outputs. Effective engagement is characterized by meaningful interaction, shared responsibility, and mutual benefit among all partners.\n\n**Background:** The concept of engagement has evolved in surveillance practice, particularly in emphasizing community involvement. Engaged communities participate in various aspects of surveillance, from identifying priorities to collecting data and using results. Evidence of engagement includes consistent participation, valuable feedback, system adaptation based on community input, and local ownership of activities. Modern approaches recognize that engagement should include diverse communities, especially those most affected by health conditions under surveillance. This shift values equitable partnerships where communities are active collaborators rather than simply sources of data.\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Equity data {#equity-data}\n\n**ID:** equityData\n\n**See also:** data collection\n\n**Synonyms:** **Exact:** TBD • **Broad:** TBD\n\n<hr class=\"header-separator\">\n\n**Definition:** Whether race/ethnicity data, measures of racism, and valid stigma indicators are captaured by the data collection of a surveillance system.\n\n**Description:** TBD\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## False-alarm rate {#false-alarm-rate}\n\n**ID:** falseAlarmRate\n\n**See also:** [specificity](#specificity)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The proportion of negative events (e.g. non-outbreak periods) incorrectly classified as events (outbreaks).\n\n**Description:** The false-alarm rate is the proportion of non-events incorrectly flagged as events, and is the inverse of specificity. While some false positives are expected, a high rate can lead to wasted resources, meanwhile a low rate may indicate poor sensitivity, risking missed outbreaks. Therefore, the proportion of false alarms (i.e., false positives) should be balanced with sensitivity, and assessed with respect to overall sensitivity.This term is positively correlated with sensitivity and cost and should be evaluated alongside overall specificity.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Feasibility {#feasibility}\n\n**ID:** feasibility\n\n**See also:** [sustainability](#sustainability)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Surveillance strategies and methods are suitable and applicable to the surveillance objectives and available resources.\n\n**Description:** Feasibility evaluates whether a surveillance system is realistic and sustainable given available resources, expertise, and objectives. It considers whether system strategies align with public health priorities and if necessary external resources (e.g., committees, affiliations) are in place. The capability of staff, including multidisciplinary expertise, is essential for system operation and maintenance. Feasibility also assesses the ability to track surveillance events over time using available data sources and whether the resources are sufficient to support all system components (data collection, analysis, dissemination) effectively and efficiently.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Flexibility {#flexibility}\n\n**ID:** flexibility\n\n**See also:** [portability](#portability)\n\n**Synonyms:** **Exact:** adaptability • **Broad:** scalability\n\n<hr class=\"header-separator\">\n\n**Definition:** The ability of the system to adapt to changing information needs or operating conditions, including scaling up or down as necessary, with little additional time, personnel, or funds.\n\n**Description:** Flexibility is the system’s ability to adapt to new information needs, emerging threats, or changing resources with minimal disruption. This may involve modifying data sources, case definitions, or reporting mechanisms. Standardized formats and modular designs support flexibility. It is closely related to sustainability, resilience, and system responsiveness.\n\n**Background:** The concept of flexibility was identified 39 times in our scoping review. The concept was first introduced by Thacker et al., 1988 (A method for evaluating systems of epidemiological surveillance) and has since been consistently included for the evaluation of public health, animal health, and One Health surveillance systems. The definition has remained highly consistent within these fields, although occasionally being refered to as &quot;adaptabilit&quot;y or &quot;adaptability to changes&quot;.\n\n**Measurement:** Flexibility is probably best evaluated retrospectively by observing how a system has responded to a new demand. Unless efforts have been made to adapt the public health surveillance system to another disease (or other health-related event), a revised case definition, additional data sources, new information technology, or changes in funding, assessing the flexibility of that system might be difficult. In the absence of practical experience, the design and workings of a system can be examined. Simpler systems might be more flexible (i.e., fewer components will need to be modified when adapting the system for a change in information needs or operating conditions).\n\n<hr class=\"term-separator\">\n\n## Governance {#governance}\n\n**ID:** governance\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The coordination, leadership, and operational effectiveness of surveillance systems, ensuring clear roles, shared responsibilities, formalized collaboration strategies, inclusive decision-making, resource allocation, and effective communication to achieve surveillance objectives across sectors and disciplines.\n\n**Description:** Governance refers to the coordination, leadership, and operational effectiveness of a surveillance system. Strong governance ensures clear roles, shared responsibilities, inclusive decision-making, and formalised collaboration across sectors and disciplines. It involves defined strategic goals, operational structures (e.g., steering committees), and documented agreements between institutions. Effective governance enhances communication, resource allocation, and accountability. It is closely linked to collaboration, and is critical for implementing One Health approaches. It has a positive effect on all other evaluation terms.\n\n**Background:** TBD\n\n**Measurement:** Measurement considers leadership, coordination mechanisms, representation, and availability of supporting resources.\n\n<hr class=\"term-separator\">\n\n## Granularity {#granularity}\n\n**ID:** granularity\n\n**See also:** completeness\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The level of detail provided in input sources and output data of the surveillance system.\n\n**Description:** Granularity refers to the level of detail in both input and output data within a surveillance system. Input granularity indicates how detailed the data sources are, such as individual data points or aggregated tables. Mismatched granularity between the required input for event-based biosurveillance and available data can impact the accuracy of results. For output, granularity reflects how detailed the results are, whether presented as a single data field, a table, or a graphic. Higher granularity in output can help identify specific trends, populations, or infection regions, but lower-detail outputs may not always be able to represent high-detail data.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Historical data {#historical-data}\n\n**ID:** historicalData\n\n**See also:** accessibility (data); data quality; data management and storage\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Refers to the quality and accessibility of archived data.\n\n**Description:** Historical data refers to the quality, completeness, and accessibility of archived surveillance information. It is especially important for demonstrating disease freedom, monitoring trends in endemic diseases, and supporting epidemiological research. This term is closely linked to data management and storage, and repeatability. Key considerations include the number of years stored, data reliability, ease of access and analysis, and availability of metadata explaining changes over time. Effective use of historical data enhances surveillance value and enables long-term analysis and planning.\n\n**Background:** TBD\n\n**Measurement:** Questions to consider include: * How many years of data are stored? * How complete and reliable are the data? * Are the data stored in a way that allows the required interrogation and analysis? * Is there a summary overview of the data and collection methods explaining key idiosyncrasies of the data and changes to the data or collection methods over time? * What use is currently made of historical surveillance data?\n\n<hr class=\"term-separator\">\n\n## Impact {#impact}\n\n**ID:** impact\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** usefulness • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** N/A\n\n**Description:** Impact measures the tangible benefits a surveillance system provides. It reflects how surveillance outputs influence decisions, policy, and interventions, including changes in behavior, protocols, or disease control. High-impact systems provide timely, complete, and representative data aligned with stakeholder and policy needs. Impact is strengthened by  by terms such as sensitivity, timeliness, data completeness, and representativeness, and is demonstrated through concrete actions taken and outcomes achieved based on surveillance findings.\n\n**Background:** TBD\n\n**Measurement:** Evaluating the impact involves examining real-world examples where the system’s data has influenced public health actions, such as policy changes or control measures. This assessment often requires retrospective analysis and gathering input from key stakeholders to understand the system's effectiveness. Key questions include how well the system’s objectives align with policy and industry needs, how outputs are utilized, and whetherstakeholder information needs have been met. Additionally, the evaluation should examine the system's influence on the development of disease control policies, the prioritization of health threats, and the early detection or mitigation of outbreaks. It is also important to detail actions taken based on the system’s findings, such as changes in protocols or behaviors. The evaluation should assess the extent to which the system’s objectives have been achieved, providing a clear understanding of its impact on public health and disease management. Retrospective assessments and stakeholder feedback help evaluate whether the system’s outputs have guided effective actions. Regular review of system performance and stakeholder input helps ensure continued relevance and effectiveness in addressing public health threats. Retrospective assessment through stakeholder input helps capture real-world benefits, such as improved disease detection, control, or prioritization.\n\n<hr class=\"term-separator\">\n\n## Interoperability {#interoperability}\n\n**ID:** interoperability\n\n**See also:** accessibility (data); data management and storage\n\n**Synonyms:** **Exact:** compatibility, integration • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Compatibility with and ability to integrate data from other sources and surveillance components.\n\n**Description:** Interoperability helps ensure timely, efficient data exchange across sectors, disciplines, and administrative entities. Interoperability supports joint analysis, enhances flexibility, and improves sustainability. Effective interoprability requires standardized data formats, unique identifiers (e.g., sample or location IDs), and reliable data-sharing mechanisms. Challenges include fragmented or duplicative systems, lack of digital integration, or a lack of formal communication pathways. The importance of interoperability may depend on how essential inter-system data exchange is to the overall usefulness and effectiveness of the surveillance system.\n\n**Background:** TBD\n\n**Measurement:** Description of the elements that are integrated in the surveillance system (Focus groups or individual interviews). Evaluation of the relevance and rationale of the integration with regards to surveillance objectives (Focus groups or individual interviews). Evaluation of the governance and multi-sectoral collaboration in the system (Focus groups or individual interviews, complementary tools). Overall evaluation of the level of integration (Semi-quantitative measurement scale, complementary tools).\n\n<hr class=\"term-separator\">\n\n## Laboratory and sample management {#laboratory-and-sample-management}\n\n**ID:** laboratoryAndSampleManagement\n\n**See also:** technical competence and training\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Whether testing is carried out using appropriate methods with quality assurance scheme and timely and accurate delivery of results.\n\n**Description:** Laboratory and sample management evaluates the integration, reliability, and quality of diagnostic laboratory functions within the surveillance system. Key considerations include the implementation of systematic quality management practices, participation in proficiency testing, and adherence to international standards (e.g., ISO 9001, ISO 17025). Effective sample handling, timely analysis, and standardized protocols and harmonization of techniques across laboratories support data comparability. Laboratory and sample management is also closely related to technical competence and training.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Likelihood ratio of a positive test {#likelihood-ratio-of-a-positive-test}\n\n**ID:** likelihoodRatioOfAPositiveTest\n\n**See also:** TBD\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Ratio of the probability of a surveillance system detecting an infected individual to the probability of the system incorrectly identifying them as infected when they are in fact not.\n\n**Description:** Likelihood ratios do not vary with disease prevalence and so are stable expressions of system performance.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Multiple utility {#multiple-utility}\n\n**ID:** multipleUtility\n\n**See also:** [flexibility](#flexibility); [portability](#portability)\n\n**Synonyms:** **Exact:** multiple hazard • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The ability of a surveillance system to capture information on several hazards; measure of how generic the system is.\n\n**Description:** Multiple utility describes a surveillance system's capacity to collect information on several hazards, reflecting how generic and adaptable it is. Systems with high multiple utility can support various surveillance objectives, improving cost-effectiveness and overall value. This term is positively linked to acceptability, sustainability, flexibility, and cos. Both realised and potential multiple utility should be assessed to identify opportunities for improvement. Systems with broad, representative designs generally offer greater multiple utility than those narrowly focused on specific risks.\n\n**Background:** TBD\n\n**Measurement:** An assessment of multiple utility should consider: - What additional information is or could be gathered during sample collection (eg on animal health or husbandry and demographics)? - What other types of samples are or could be collected at the time of sampling? - What other diseases are or could be tested for with the samples collected? - How long are samples stored following testing and could they be used for other purposes (including other research purposes)? For a surveillance system to offer value to other diseases or information needs, the objectives and processes of the system should be aligned to other systems. So it may be expected that more simple systems are likely to have more potential for multiple utility. For example, a simple random survey of holdings, repeated annually and with good coverage and representativeness could be useful for various diseases; whereas a risk-based design aimed at a specific threat may be of limited value for other diseases with differing epidemiology.\n\n<hr class=\"term-separator\">\n\n## Mutual benefit {#mutual-benefit}\n\n**ID:** mutualBenefit\n\n**See also:** [collaboration](#collaboration)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The capability of the surveillance system to create opportunities where collaborations can benefit both the system and its partners/collaborators.\n\n**Description:** Mutual Benefit refers to the extent to which all collaborators gain value from participating in the surveillance system. This includes the availability of shared data infrastructure, clarity in roles and responsibilities, and the fostering of trusted relationships. Mutual understanding of privacy, confidentiality, and security processes supports sustained cooperation. Systems that enable data exchange and define partner benefits are more likely to promote long-term collaboration and stakeholder engagement.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Negative predictive value (NPV) {#negative-predictive-value-npv}\n\n**ID:** negativePredictiveValue\n\n**See also:** positive predictive value\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The probability that no health event is present given that no health event is detected.\n\n**Description:** Negative predictive value (NPV) reflects the likelihood that a negative surveillance result truly indicates absence of condition, hazard, or event under surveillance. A high NPV suggests few false negatives and supports confidence in the system's ability to rule out threats when no signal is detected. NPV is influenced by the system's sensitivity, specificity, and the prevalence of the condition in the population. IIt applies acros various public health surveillance modalities, including lagroatory, syndomic, environmental, and even-based approaches, and should be considered alongside sensitivity and positive predictive value.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Performance monitoring and evaluation {#performance-monitoring-and-evaluation}\n\n**ID:** performanceMonitoringAndEvaluation\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The routine use of performance indicators to monitor surveillance system performance, and the periodic use of internal or external evaluations to assess system outputs in relation to stated objectives.\n\n**Description:** Performance monitoring and evaluation assesses the extent to which the surveillance system uses performance indicators and scheduled evaluations to track progress, measure effectiveness, and ensure accountability. This includes regular internal monitoring and external assessments of system components, including data quality, timeliness, sensitivity, and stakeholder collaboration. Performance monitoring and evaluation supports system transparency, enables corrective actions, and contributes to long-term improvement. It is closely linked to effectiveness, efficiency, impact, and sustainability.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Portability {#portability}\n\n**ID:** portability\n\n**See also:** [flexibility](#flexibility)\n\n**Synonyms:** **Exact:** generalizability • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The ease with which a surveillance system can be replicated or implemented in a different context or setting.\n\n**Description:** Portability describes how easily a surveillance system can be adopted or adapted in other settings or jurisdictions. High portability is supported by standardized formats, clear documentation, modular design, and minimal dependence on local infrastructure or specialized personnel. Systems that are portable enable broader uptake, facilitate scalability, and enhance knowledge sharing. Portability is positively associated with attributes such as simplicity, flexibility, and sustainability.\n\n**Background:** TBD\n\n**Measurement:** Examples should be provided of the deployment of similar systems in other settings, and the experience of those efforts should be described. In the absence of examples, features of the system that might support or detract from portability should be described.\n\n<hr class=\"term-separator\">\n\n## Positive predictive value (PPV) {#positive-predictive-value-ppv}\n\n**ID:** positivePredicitiveValue\n\n**See also:** negative predictive value\n\n**Synonyms:** **Exact:** N/A • **Broad:** sensitivity\n\n<hr class=\"header-separator\">\n\n**Definition:** The proportion of reported cases that actually have the health-related event under surveillance.\n\n**Description:** Positive Predictive Value (PPV) is the proportion of cases identified by a surveillance system that are true cases. A high PPV indicates fewer false positives, which helps minimize unnecessary investigtations and conserves resources. PPV is influenced by the specificity and sensitivity of case definitions, as well as the prevalence of the condition in the population. As sensitivity and PPV approach 100%, a system is more likely to be representative of the population with the event under surveillance. However, as sensitivity increases, PPV may decrease; therefore, a balance should be maintained between prioritizing early detection and accuracy. PPV may also influenced by data quality, communication, and reporting practices.\n\n**Background:** TBD\n\n**Measurement:** To assess PVP, external data sources, such as medical records, health claims, or registries, are often used as benchmarks to verify the true cases identified by the surveillance system. The assessment of sensitivity and of PVP provide different perspectives regarding how well the system is operating. Depending on the objectives of the public health surveillance system, assessing PVP whenever sensitivity has been assessed might be necessary (47-50,53 ). In this report, PVP is represented by A/(A+B) (Table 3). Evaluating Positive Predictive Value (PVP) involves assessing the accuracy of case detection and outbreak identification based on reported data. This can be done on two levels: individual case detection and epidemic detection. For case detection, the PVP is determined by confirming reported cases through investigation and comparing the number of true cases to the total number of reported cases. This requires keeping records of all interventions triggered by the surveillance system, such as the number of case investigations conducted and the proportion of cases confirmed. For epidemic detection, the PVP is assessed by evaluating the proportion of detected outbreaks that are true epidemics. This can involve reviewing personnel activity reports, travel records, and telephone logbooks to determine if resources are being misdirected due to false-positive case reports. Additionally, external data sources, such as medical records or registries, may be used to validate reported cases and improve the accuracy of PVP calculations. To obtain a comprehensive understanding, it may be necessary to assess PVP for different data sources, system data fields, and specific health-related events. Regular analysis of these factors ensures that the system effectively uses resources and minimizes the impact of false-positive results.\n\n<hr class=\"term-separator\">\n\n## Precision {#precision}\n\n**ID:** precision\n\n**See also:** [accuracy](#accuracy); bias\n\n**Synonyms:** **Exact:** N/A • **Broad:** repeatability, consistency\n\n<hr class=\"header-separator\">\n\n**Definition:** The degree to which a numerical estimate is narrowly defined, typically indicated by the tightness of its confidence interval.\n\n**Description:** Precision reflects the level of statistical uncertainty around an estimate, with narrower confidence intervals indicating higher precision. It is influenced by sample size, confidence level, variability in the data, and overall data quality. Precision is important for interpreting the reliability of surveillance indicators and comparing system performance over time or across regions.\n\n**Background:** TBD\n\n**Measurement:** Evaluation of statistical stability (precision) involves calculation of relative standard error of the primary estimate\n\n<hr class=\"term-separator\">\n\n## Preparedness {#preparedness}\n\n**ID:** preparedness\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Preparedness corresponds to the set of actions that are taken as precautionary measures in the face of potential hazard-related issues.\n\n**Description:** Preparedness refers to the extent to which a surveillance system is equipped to respond effectively to public health emergencies. This includes having written procedures, emergency trained personnel, and readily available resources to support timely action. A well-prepared system can detect, report, and respond to emerging threats quickly and efficiently. The preparedness term is closely linked to timeliness, stability, and sustainability, as it depends on ongoing capacity, planning, and coordination.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Redundancy {#redundancy}\n\n**ID:** redundancy\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The ability of the surveillance system to operate in the face of component loss or degradation.\n\n**Description:** Redundancy refers to the presence of backup systems or components—such as staff, IT infrastructure, data sources, or interagency agreements—that ensure continued surveillance operations if one part fails. It supports system stability and resilience, allowing for uninterrupted performance during disruptions. Without redundancy, the failure of a single component can compromise data collection, analysis, or reporting. Redundancy is closely related to stability, robustness, and sustainability, as it enhances the system’s ability to maintain functionality and deliver reliable outputs under various conditions.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Relevance {#relevance}\n\n**ID:** relevance\n\n**See also:** effectivness; [usefulness](#usefulness)\n\n**Synonyms:** **Exact:** pertienence • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The degree to which functions and activities in the surveillance system are relevant to its objectives and priorities.\n\n**Description:** Relevance assesses how well a surveillance system’s functions, activities, and data align with its objectives, priorities, and user needs. It examines whether the system addresses key data gaps, supports decision-making, and reflects epidemiological, socio-political, and economic contexts. Relevance also involves ensuring collaborative activities (e.g., sampling, lab testing, data sharing) match the system’s goals and stakeholder expectations. High relevance enhances system usefulness, acceptability, and sustainability by focusing resources on priority surveillance needs.\n\n**Background:** TBD\n\n**Measurement:** Evaluation considers if unnecessary data collection is minimized and if performance indicators are linked to objectives.\n\n<hr class=\"term-separator\">\n\n## Repeatability {#repeatability}\n\n**ID:** repeatability\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** – • **Broad:** consistency, precision\n\n<hr class=\"header-separator\">\n\n**Definition:** How consistently the study results can be reproduced over time.\n\n**Description:** Repeatability is a key concept in validating diagnostic tests and is closely tied to precision. In the context of a surveillance system, it also relates to the terms/attributes of historical data, stability, and sustainability. A surveillance system that excels in repeatability generates data that can be consistently compared across years, with clear documentation of any changes in data collection methods or variables over time. This ensures that variations in the data are well-understood and attributable to actual trends rather than inconsistencies in data collection, facilitating accurate longitudinal analysis and comparison.\n\n**Background:** TBD\n\n**Measurement:** One might consider changes to legislation; changes to diagnostic methods, including improvements of adoption of new technology; changes to surveillance design; or influences on disease reporting behaviour in passive surveillance activities. - How have these impacted on the comparability of surveillance data over the time period of interest? - Have these influences been identified and examined and can they be accommodated in interpretation of the surveillance data?\n\n<hr class=\"term-separator\">\n\n## Representativeness {#representativeness}\n\n**ID:** representativeness\n\n**See also:** [coverage](#coverage)\n\n**Synonyms:** **Exact:** N/A • **Broad:** coverage?\n\n<hr class=\"header-separator\">\n\n**Definition:** The extent to which data adequately represent the population under surveillance and relevant sub-populations by time, place, population demographics and socio-demographics.\n\n**Description:** Representativeness refers to how well surveillance data reflect the characteristics of the population of interest, allowing findings to be generalized without systematic bias. It considers factors like geography, demographics, and clinical presentation. While sensitivity focuses on detecting cases, representativeness ensures that the distribution of cases mirrors reality. High coverage across regions and groups supports inclusivity, while limited or uneven data can lead to selection or ascertainment bias. The representativeness of a surveillance system is related to both coverage and bias. Representativeness is also closely linked to terms such as sensitivity, precision, and data quality, and is essential for drawing accurate conclusions and informing effective public health planning.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Responsiveness (processes) {#responsiveness-processes}\n\n**ID:** responsiveness\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** –\n\n<hr class=\"header-separator\">\n\n**Definition:** Responsiveness refers to the system's ability to promptly address user requests and the willingness of support staff to assist with inquiries and provide necessary services.\n\n**Description:** Responsiveness measures how quickly and effectively a surveillance system addresses user requests and support needs. It reflects the willingness and ability of staff to assist with inquiries, troubleshoot issues, and provide necessary services. High responsiveness supports timely data reporting, improves user satisfaction, and enhances system acceptability.  Responsiveness is closely linked to timeliness and acceptability, ensuring the system remains user-friendly and operationally efficient.\n\n**Background:** TBD\n\n**Measurement:** Evaluation includes user feedback on response times and support quality, as well as tracking the availability and effectiveness of help resources.\n\n<hr class=\"term-separator\">\n\n## Robustness {#robustness}\n\n**ID:** robustness\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The ability of the surveillance system to produce acceptable outcomes over a range of assumptions about uncertainty by maximising the reliability of an adequate outcome.\n\n**Description:** Robustness refers to a surveillance system's ability to consistently generate reliable results over time, even under varying conditions and uncertainties. It ensures stable, accurate outcomes regardless of environmental or data changes. Robust systems are closely linked to precision, as both require dependable results. Additionally, robustness ties into consistency, flexibility, and portability, allowing the system to adapt to diverse contexts. A robust surveillance system is crucial for providing reliable data that supports long-term public health decisions and interventions, making it vital for effective and sustainable public health responses.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Security {#security}\n\n**ID:** security\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The privacy and data confidentiality measures required to prevent surveillance data compromise, based on relevant standards and guidelines.\n\n**Description:** Security in a surveillance system involves safeguarding the privacy and confidentiality of data to prevent unauthorized access or breaches. Security measures should be reviewed regularly and adapted to emerging threats. Strong security practices ensures data integrity and builds trust between actors/stakeholders of the surveillance system.\n\n**Background:** TBD\n\n**Measurement:** Surveillance system security policies and practices should be reviewed to ensure that security levels and procedures for surveillance system data or system access are defined and enforced; data use and release policy and protocol is available; and access to the surveillance system software application is controlled.\n\n<hr class=\"term-separator\">\n\n## Sensitivity {#sensitivity}\n\n**ID:** sensitivity\n\n**See also:** [specificity](#specificity)\n\n**Synonyms:** **Exact:** recall (Morbey 2021), completeness (Groseclose 2010) • **Broad:** positive predictive value\n\n<hr class=\"header-separator\">\n\n**Definition:** The system’s ability to accurately identify and capture all true cases or events of interest within the target population\n\n**Description:** Sensitivity can be assessed at three levels: Case detection sensitivity – the proportion of actual cases (individuals or herds) with the condition that are correctly identified by the system; Outbreak detection sensitivity – the likelihood that the system will detect a significant increase in disease incidence, provided a clear definition of an outbreak exists; and Presence detection sensitivity – the probability that the system will detect the disease if it is present at or above a specified prevalence level in the population.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Simplicity {#simplicity}\n\n**ID:** simplicity\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Refers to the surveillance system structure, ease of operation, and flow of data through the system.\n\n**Description:** Simplicity should be inherent in the system as a whole, as well as each component (case definition, reporting procedures, etc.), to make it easy to understand and implement. In general, a surveillance system should be as simple as possible while still meeting its objectives. A simple system typically enhances flexibility, reduces resource requirements, and contributes to better timeliness. Simplicity also positively influences acceptability and engagement, as actors/stakeholders are more likely to adopt and participate in a system that is easy to use and understand. Simplicity also supports sustainability by easing training, maintenance, and management, ultimately leading to more accurate and comprehensive data.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Specificity {#specificity}\n\n**ID:** specificity\n\n**See also:** false-alarm rate; [sensitivity](#sensitivity)\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The proportion of cases correctly identified by the system as not having the health-related event under surveillance.\n\n**Description:** Specificity refers to a surveillance system’s ability to correctly identify non-cases, minimizing false positives. It complements sensitivity by ensuring that resources are not wasted on investigating non-events. High specificity is especially important in outbreak detection and in systems monitoring rare or high-impact conditions. It reduces false alarms, conserving time and funding. Specificity is influenced by factors such as case definitions and population denominators. In some contexts, a trade-off with sensitivity may be acceptable to avoid missing critical threats.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Stability {#stability}\n\n**ID:** stability\n\n**See also:** [sustainability](#sustainability)\n\n**Synonyms:** **Exact:** ability to contrinue tracking an event (sub-attribute), reliability, availability • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Stability refers to the reliability (ie, the ability to collect,manage, and provide data without failure) and availability (the ability to operate when needed) of the public health surveillancesystem.\n\n**Description:** Stability refers to the system’s ability to consistently operate and deliver results over time, even under stress or changing conditions. It includes reliability, availability, and resilience to disruptions. Stability depends on factors such as workforce capacity, secure funding, legal frameworks, and continuity plans. It is positively correlated with sustainability and acceptability.\n\n**Background:** TBD\n\n**Measurement:** Measures for determining stability can include the number or duration of unscheduled outages of the information system(s) supporting the surveillance system; the comparison of the desired and actual amount of time or resources required for the system to collect, manage, analyze, interpret, and release data from the system; or the presence or absence of continuity of operations procedures intended to maintain system performance. This attribute can be measured retrospectively by 1. Looking at the incidence of minor and major faults over a defined period of time or 2. Giving a measure of the proportion of time that the system is fully functional Assessment of this attribute will benefit from consultation with those involved in the generation, management and analysis of surveillance data. If performance indicators have been implemented in the surveillance process, historical data from these will give a good insight into the ongoing functioning of the system.\n\n<hr class=\"term-separator\">\n\n## Sustainability {#sustainability}\n\n**ID:** sustainability\n\n**See also:** [stability](#stability); feasability; N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The ability of the surveillance system to be ongoing in the long term.\n\n**Description:** Sustainability refers to the system’s ability to operate effectively over time with consistent support. This includes having skilled personnel, sufficient laboratory capacity for timely sample processing, and clearly defined responsibilities for resource provision. Resources should align with current operational needs. Sustainability is closely linked to usefulness and timeliness, emphasizing practical procedures, adequate budgeting, and long-term system maintenance. It is dependednt on the continued engagement from stakeholders/actors involved in core surveillance functions, as well as the acceptability of end-users of the surveillance system.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## System description {#system-description}\n\n**ID:** systemDescription\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** TBD\n\n**Description:** N/A\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Technical competence and training {#technical-competence-and-training}\n\n**ID:** technicalCompetenceAndTraining\n\n**See also:** laboratory and sample management\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Refers to the technical skills of the personnel involved in the surveillance system, including access to relevant training.\n\n**Description:** The combination of the technical skills, knowledge, and capacities required by personnel to effectively implement and sustain a surveillance system, supported by access to appropriate initial and ongoing training programs. It encompasses the ability to perform system functions accurately, ensure data quality, and engage in collaborative activities, thereby enhancing the overall performance and reliability of the surveillance system\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Timeliness {#timeliness}\n\n**ID:** timeliness\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The time between any two steps in the surveillance process.\n\n**Description:** Timeliness refers to the speed at which data are collected, reported, analyzed, and used to guide public health action. The required level of timeliness depends on the health event—rapid reporting is critical for acute, highly transmissible diseases, while less urgent conditions may allow for slower reporting. Timeliness is influenced by surveillance system design, resource availability, and event complexity. Delays in lab results or data flow can hinder timely response, especially in resource-limited settings. It must be balanced with accuracy and completeness and aligned with surveillance goals such as outbreak detection, trend monitoring, or long-term planning.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Traceability (data) {#traceability-data}\n\n**ID:** traceabilityData\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** N/A\n\n**Description:** TBD\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Transparency {#transparency}\n\n**ID:** transparency\n\n**See also:** N/A\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** The extent to which information can be and is shared across member agencies.\n\n**Description:** Transparency refers to how openly the surveillance system shares information about its data collection methods, analysis processes, and limitations with stakeholders and end-users. High transparency builds trust, supports data interpretation, and facilitates informed decision-making. Challenges include inconsistent or delayed data sharing, which can reduce stakeholder confidence and limit timely responses. Transparency is linked to terms like acceptability and usefulness, as clear communication of methods and constraints helps users understand system strengths and weaknesses.\n\n**Background:** TBD\n\n**Measurement:** Measurement may involve assessing frequency, clarity, and accessibility of data reports and documentation.\n\n<hr class=\"term-separator\">\n\n## Usablity {#usablity}\n\n**ID:** usability\n\n**See also:** [usefulness](#usefulness); communication dissemination\n\n**Synonyms:** **Exact:** N/A • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** Data and products produced by the surveillance system should be tailored to meet the needs of intended users.\n\n**Description:** Usability assesses whether surveillance data and products are tailored to meet the needs of intended users. High usability involves clear documentation (data dictionaries, manuals), minimal training requirements, and system workflows that align with user practices. It also considers how well data interpretations (e.g., statistics) are communicated, including accessibility features like language, cultural relevance, and format. The ability to stratify data by relevant factors (age, sex, geography) enhances usefulness. Regular user feedback guides system improvements. Usability links closely with acceptability and usefulness, influencing how effectively data informs public health actions.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n<hr class=\"term-separator\">\n\n## Usefulness {#usefulness}\n\n**ID:** usefulness\n\n**See also:** [effectiveness](#effectiveness)\n\n**Synonyms:** **Exact:** impact • **Broad:** N/A\n\n<hr class=\"header-separator\">\n\n**Definition:** [Wasn't sure how much we would want to frame this definition around public health]. The extent to which the system has benefited the end-users and led to specific public health actions, either in the form of assessments or measures. [OR] A public health surveillance system is useful if it contributes to the prevention and control of adverse health-related events, including an improved understanding of the public health implications of such events .\n\n**Description:** Usefulness reflects the system’s impact on preventing or controlling adverse health events and informing public health action. It is demonstrated through real-world outcomes, such as policy changes, early outbreak detection, or targeted interventions. Usefulness depends on how well outputs meet stakeholder needs and influence decision-making. Retrospective analysis and stakeholder input help assess alignment with health priorities. This attribute is closely tied to sensitivity, timeliness, and data completeness, which together enhance the system’s value in guiding effective, evidence-based responses.\n\n**Background:** TBD\n\n**Measurement:** TBD\n\n\n---\n\n## Markdown Rendering Issues\n\nThe following fields had issues rendering markdown:\n\n- **dataQuality** (term_background): Unclosed change tracking markup found\n\n# References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "dictionary_documentation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}