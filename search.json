[
  {
    "objectID": "figures.html#table1",
    "href": "figures.html#table1",
    "title": "Figures",
    "section": "Table 1 — Study characteristics",
    "text": "Table 1 — Study characteristics\n\n\n\n\n\n\n\n\nTable 1. Study characteristics\n\n\nStudy ID\nStudy title\nCorresponding author country\nStudy design type\nSurveillance Type\nEvaluation Framework Used\n\n\n\n\nKlaucke 1988\nGuidelines for Evaluating Surveillance Systems\nnot reported\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nAenishaenslin 2021\nEvaluating the Integration of One Health in Surveillance Systems for Antimicrobial Use and Resistance: A Conceptual Framework\nCanada\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nAlbali 2023\nComparative Performance Evaluation of the Public Health Surveillance Systems in 6 Gulf Cooperation Countries: Cross-sectional Study\nSaudi Arabia\nFormal evaluation\nPublic Health\nCDC 2001; CDC 2004\n\n\nAmato 2023\nEvaluation of the pilot wastewater surveillance for SARS-CoV-2 in Norway, June 2022 - March 2023\nNorway\nFormal evaluation\nWastewater\nCDC 2001; ECDC 2014\n\n\nAnonymous 2001\nUpdated Guidelines for Evaluating Public Health Surveillance Systems\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nArinik 2023\nAn Evaluation Framework For Comparing Epidemic Intelligence Systems\nFrance\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nAuer 2001\nCanadian Aboriginal communities: a framework for injury surveillance\nCanada\nGuidance or a descriptive framework\nPublic Health\nNo framework(s) or guidance document(s) were discussed\n\n\nAuer 2011\nThe relevance of WHO injury surveillance guidelines for evaluation: learning from the Aboriginal Community-Centered Injury Surveillance System (ACCISS) and two institution-based systems\nSweden\nGuidance or a descriptive framework\nPublic Health\nOther\n\n\nAzofeifa 2018\nEvaluating Behavioral Health Surveillance Systems\nN/A\nGuidance or a descriptive framework\nPublic Health\nNo framework(s) or guidance document(s) were discussed\n\n\nBabaie 2015\nPerformance Assessment of Communicable Disease Surveillance in Disasters: A Systematic Review\nIran\nDomains, themes\nPublic Health\nNot applicable\n\n\nBaboMartins 2017\nEconomics of zoonoses surveillance in a One Health context: an assessment of Campylobacter surveillance in Switzerland\nUnited Kingdom\nFormal evaluation\nOne Health\nOther\n\n\nBaker 2006\nGlobal Public Health Surveillance under New International Health Regulations\nNew Zealand\nFormal evaluation\nPublic Health\nCDC 2001\n\n\nBennani 2021\nEvaluating Integrated Surveillance for Antimicrobial Use and Resistance in England: A Qualitative Study\nUnited Kingdom\nFormal evaluation\nOne Health\nOther\n\n\nBingle 2005\nAn Evaluation of the Ontario Rapid Risk Factor Surveillance System\nCanada\nGuidance or a descriptive framework\nPublic Health\nCDC 2001\n\n\nBordier 2019\nOne Health Surveillance: A Matrix to Evaluate Multisectoral Collaboration\nVietnam\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nBordier 2020\nCharacteristics of One Health surveillance systems: A systematic literature review\nVietnam\nDomains, themes\nOne Health\nNot applicable\n\n\nBordier 2022\nEvaluation of Collaboration in a Multisectoral Surveillance System: The ECoSur Tool\nVietnam\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nBravata 2004\nEvaluating Detection and Diagnostic Decision Support Systems for Bioterrorism Response\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nBuehler 2004\nFramework for Evaluating Public Health Surveillance Systems for Early Detection of Outbreaks: Recommendations from the CDC Working Group\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nBuehler 2008\nEvaluation of surveillance systems for early epidemic detection\nunknown\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nCalba 2015\nSurveillance systems evaluation: a systematic review of the existing approaches\nFrance\nDomains, themes\nOther\nNot applicable\n\n\nCalba 2015\nApplying participatory approaches in the evaluation of surveillance systems: A pilot study on African swine fever surveillance in Corsica\nBelgium\nFormal evaluation\nOther\nOASIS; Other\n\n\nCalba 2016\nThe Added-Value of Using Participatory Approaches to Assess the Acceptability of Surveillance Systems: The Case of Bovine Tuberculosis in Belgium\nFrance\nFormal evaluation\nOther\nOASIS; Other\n\n\nCameron 2020\nQuantification of the sensitivity of early detection surveillance\nFrance\nDomains, themes\nPublic Health\nNot applicable\n\n\nChoi 2008\nEnhancing global capacity in the surveillance, prevention, and control of chronic diseases: seven themes to consider and build upon\nCanada\nDomains, themes\nPublic Health\nNot applicable\n\n\nClara 2020\nDeveloping monitoring and evaluation tools for event-based surveillance: experience from Vietnam\nUnited States of America\nDomains, themes\nPublic Health\nNo framework(s) or guidance document(s) were discussed\n\n\nCollineau 2022\nEvaluation of the French surveillance system for epidemiological surveillance of antimicrobial resistance in the community and nursing homes\nFrance\nFormal evaluation\nOne Health\nOASIS\n\n\nCorley 2012\nAssessing the Continuum of Event-Based Biosurveillance Through an Operational Lens\nUnited States of America\nGuidance or a descriptive framework\nOther\nNot applicable\n\n\nCrawley 2021\nUsing Timeliness Metrics to Track Progress and Identify Gaps in Disease Surveillance\nUnited States of America\nDomains, themes\nPublic Health\nNot applicable\n\n\nCutts 1993\nSurveillance for the Expanded Programme on Immunization\nUnited Kingdom\nDomains, themes\nPublic Health\nNot applicable\n\n\nDeclich 1994\nPublic health surveillance: historical origins, methods and evaluation\nCanada\nDomains, themes\nPublic Health\nNot applicable\n\n\nDelRioVilas 2022\nHealth Surveillance Evaluation in the Policy Cycle\nFrance\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nDente 2018\nIntegrated surveillance and risk assessment for arbovirus infections: recommendations for enhancing One Health in the Mediterranean Region: MediLabSecure Strategic Document 2018\nnot reported\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nDrewe 2012\nEvaluation of animal and public health surveillance systems: a systematic review\nUnited Kingdom\nDomains, themes\nOther\nNot applicable\n\n\nDrewe 2015\nSERVAL: A New Framework for the Evaluation of Animal Health Surveillance\nUnited Kingdom\nGuidance or a descriptive framework\nOther\nNot applicable\n\n\nElAllaki 2013\nConceptual evaluation of population health surveillance programs: Method and example\nCanada\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nFord 2021\nAdequacy of Existing Surveillance Systems to Monitor Racism, Social Stigma and COVID Inequities: A Detailed Assessment and Recommendations\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nCDC 2001; Other\n\n\nGarcia-Vozmediano 2022\nA One Health Evaluation of the Surveillance Systems on Tick-Borne Diseases in the Netherlands, Spain and Italy\nItaly\nFormal evaluation\nOne Health\nOther\n\n\nGoutard 2022\nTHE USE OF PARTICIPATORY METHODS IN THE EVALUATION OF HEALTH SURVEILLANCE SYSTEMS\nFrance\nGuidance or a descriptive framework\nOther\nNot applicable\n\n\nGroseclose 2010\nEvaluating Public Health Surveillance\nnot reported\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nGroseclose 2013\nEvaluation of syndromic surveillance systems that use healthcare data\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nGroseclose 2017\nPublic Health Surveillance Systems: Recent Advances in Their Use and Evaluation\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nHall 2007\nSetting Standards and an Evaluation Framework for Human Immunodeficiency Virus/Acquired Immunodeficiency Syndrome Surveillance\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nHarper 2011\nImproving Aboriginal health data capture: evidence from a health registry evaluation\nCanada\nFormal evaluation\nPublic Health\nCDC 2001\n\n\nHaworth-Brockman 2021\nOne Health Evaluation of Antimicrobial Use and Resistance Surveillance: A Novel Tool for Evaluating Integrated, One Health Antimicrobial Resistance and Antimicrobial Use Surveillance Programs\nCanada\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nHendrikx 2011\nOASIS: an assessment tool of epidemiological surveillance systems in animal health and food safety\nFrance\nGuidance or a descriptive framework\nOther\nNot applicable\n\n\nHerida 2016\nEconomic Evaluations of Public Health Surveillance Systems: a Systematic Review\nFrance\nDomains, themes\nPublic Health\nNot applicable\n\n\nHoinville 2013\nProposed terms and concepts for describing and evaluating animal-health surveillance systems\nUnited Kingdom\nDomains, themes\nOther\nNot applicable\n\n\nInnes 2022\nEnhancing global health security in Thailand: Strengths and challenges of initiating a One Health approach to avian influenza surveillance\nUnited States of America\nFormal evaluation\nOne Health\nCDC 2001; Other\n\n\nJajosky 2004\nEvaluation of reporting timeliness of public health surveillance systems for infectious diseases\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNo framework(s) or guidance document(s) were discussed\n\n\nLucero-Obusan 2022\nPublic health surveillance in the U.S. Department of Veterans Affairs: evaluation of the Praedico surveillance system\nUnited States of America\nFormal evaluation\nPublic Health\nCDC 2001\n\n\nMader 2021\nOASIS evaluation of the French surveillance network for antimicrobial resistance in diseased animals (RESAPATH): success factors underpinning a well-performing voluntary system\nFrance\nFormal evaluation\nOne Health\nOASIS\n\n\nMarbus 2020\nExperience of establishing severe acute respiratory surveillance in the Netherlands: Evaluation and challenges\nNetherlands\nFormal evaluation\nPublic Health\nCDC 2001; ECDC 2014; Other\n\n\nMitchell 2009\nThe development of an evaluation framework for injury surveillance systems\nAustralia\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nMorbey 2021\nEvaluating multi-purpose syndromic surveillance systems - a complex problem\nUnited Kingdom\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nMuellner 2018\nSurF: an innovative framework in biosecurity and animal health surveillance evaluation\nNew Zealand\nGuidance or a descriptive framework\nOther\nNot applicable\n\n\nPaternoster 2017\nThe degree of One Health implementation in the west nile virus integrated surveillance in Northern Italy, 2016\nItaly\nFormal evaluation\nOne Health\nOther\n\n\nPavlin 2013\nSyndromic surveillance for infectious diseases\nUnited States of America\nDomains, themes\nPublic Health\nNot applicable\n\n\nPelican 2019\nSynergising tools for capacity assessment and One Health operationalisation\nUnited States of America\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nPeyre 2019\nThe RISKSUR EVA tool (Survtool): A tool for the integrated evaluation of animal health surveillance systems\nFrance\nGuidance or a descriptive framework\nOther\nNot applicable\n\n\nPeyre 2022\nSynthesis-evaluate to better inform: A way to strengthening health surveillance systems\nFrance\nOther\nOne Health\nNot applicable\n\n\nReintjes 2007\nBenchmarking national surveillance systems: a new tool for the comparison of communicable disease surveillance and control in Europe\nGermany\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nRomano 2022\nAn Evaluation of Syndromic Surveillance-Related Practices Among Selected State and Local Health Agencies\nUnited States of America\nDomains, themes\nPublic Health\nNo framework(s) or guidance document(s) were discussed\n\n\nRüegg 2022\nGuidance for evaluating integrated surveillance of antimicrobial use and resistance\nSwitzerland\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nSahal 2009\nCommunicable diseases surveillance lessons learned from developed and developing countries: Literature review\nDenmark\nDomains, themes\nPublic Health\nNot applicable\n\n\nSandberg 2021\nAssessment of Evaluation Tools for Integrated Surveillance of Antimicrobial Use and Resistance Based on Selected Case Studies\nDenmark\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nSosin 2003\nDraft Framework for Evaluating Syndromic Surveillance Systems\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nTegegne 2023\nOH-EpiCap: a semi-quantitative tool for the evaluation of One Health epidemiological surveillance capacities and capabilities\nFrance\nGuidance or a descriptive framework\nOne Health\nNot applicable\n\n\nThacker 1988\nA METHOD FOR EVALUATING SYSTEMS OF EPIDEMIOLOGICAL SURVEILLANCE\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nThacker 1988\nPUBLIC HEALTH SURVEILLANCE IN THE UNITED STATES\nUnited States of America\nDomains, themes\nPublic Health\nNot applicable\n\n\nVasconcelosGioia 2021\nInforming resilience building: FAO's Surveillance Evaluation Tool (SET) Biothreat Detection Module will help assess national capacities to detect agro-terrorism and agro-crime\nItaly\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nVelasova 2015\nEvaluation of the usefulness at national level of the dairy cattle health and production recording systems in Great Britain\nnot reported\nFormal evaluation\nOther\nSERVAL; Other\n\n\nWorldHealthOrganization 2004\nOverview of the WHO framework for monitoring and evaluating surveillance and response systems for communicable diseases\nnot reported\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nYang 2022\nUnderstanding occupational safety and health surveillance: expert consensus on components, attributes and example measures for an evaluation framework\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nPeyre 2022\nPrinciples for evaluation of one health surveillance: The EVA book\nFrance\nGuidance or a descriptive framework\nOne Health\nThe RISKSUR EVA tool (Survtool); Other\n\n\nRibeiro 2019\nOvercoming challenges for designing and implementing the One Health approach: A systematic review of the literature\nNetherlands\nDomains, themes\nOne Health\nNot applicable\n\n\nMeynard 2008\nProposal of a framework for evaluating military surveillance systems for early detection of outbreaks on duty areas\nFrench Guiana\nGuidance or a descriptive framework\nPublic Health\nHealth Canada 2004; Other\n\n\nMargevicius 2014\nAdvancing a Framework to Enable Characterization and Evaluation of Data Streams Useful for Biosurveillance\nUnited States of America\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\nHitziger 2021\nSystem Thinking and Citizen Participation Is Still Missing in One Health Initiatives ‚ Lessons From Fifteen Evaluations\nSwitzerland\nDomains, themes\nOne Health\nNot applicable\n\n\nNg'etich 2021\nDevelopment and validation of a framework to improve neglected tropical diseases surveillance and response at sub-national levels in Kenya\nSouth Africa\nGuidance or a descriptive framework\nPublic Health\nNot applicable\n\n\n\n\n\n\n\n\n↑ Back to top"
  },
  {
    "objectID": "figures.html#upset-plot",
    "href": "figures.html#upset-plot",
    "title": "Figures",
    "section": "Upset Plot",
    "text": "Upset Plot\n\n\n\n\n\n\n\n\n\n\n↑ Back to top"
  },
  {
    "objectID": "figures.html#framework-used",
    "href": "figures.html#framework-used",
    "title": "Figures",
    "section": "Framework used to evaluate surveillance systems",
    "text": "Framework used to evaluate surveillance systems\n\n\n\n\n\n\n\n\n\n\n↑ Back to top"
  },
  {
    "objectID": "figures.html#surv-system",
    "href": "figures.html#surv-system",
    "title": "Figures",
    "section": "Type of surveillance system",
    "text": "Type of surveillance system\n\n\n\n\n\n\n\n\n\n\n↑ Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PHES EF",
    "section": "",
    "text": "This site contains: - Dictionary – the term dictionary and documentation. - Figures – Upset plot and bar graphs.\n\nOpen Dictionary → Open Figures →\n\n\n\n\n\nDictionary\nFigures"
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "PHES EF",
    "section": "",
    "text": "Dictionary\nFigures"
  },
  {
    "objectID": "dictionary_documentation.html",
    "href": "dictionary_documentation.html",
    "title": "dictionary documentation",
    "section": "",
    "text": "ID: abilityToContinueTrackingAnEvent\nSee also: N/A\nSynonyms\n\nExact: stability (reliability)\nBroad: N/A\n\nDefinition: The ability of the surveillance system to continue tracking an event throughout the event life cycle.\nDescription: An event-based biosurveillance may be quite successful in detecting or forecasting an outbreak of disease, but it may lose its sensitivity as the event progresses (e.g., due to a high background level of indicators following a media announcement).\nBackground: This reported concept was only identified by Corley 2012 in the context of biosurveillance models and systems.\nMeasurement: N/A"
  },
  {
    "objectID": "dictionary_documentation.html#b",
    "href": "dictionary_documentation.html#b",
    "title": "dictionary documentation",
    "section": "",
    "text": "ID: abilityToContinueTrackingAnEvent\nSee also: N/A\nSynonyms\n\nExact: stability (reliability)\nBroad: N/A\n\nDefinition: The ability of the surveillance system to continue tracking an event throughout the event life cycle.\nDescription: An event-based biosurveillance may be quite successful in detecting or forecasting an outbreak of disease, but it may lose its sensitivity as the event progresses (e.g., due to a high background level of indicators following a media announcement).\nBackground: This reported concept was only identified by Corley 2012 in the context of biosurveillance models and systems.\nMeasurement: N/A"
  },
  {
    "objectID": "dictionary_documentation.html#acceptability",
    "href": "dictionary_documentation.html#acceptability",
    "title": "dictionary documentation",
    "section": "Acceptability",
    "text": "Acceptability\nID: acceptability\nSee also: engagement; collaboration\nSynonyms\n\nExact: N/A\nBroad: participation\n\nDefinition: The willingness of persons and organizations to participate in the surveillance system.\nDescription: Acceptability reflects stakeholders’ perceptions and attitudes toward the surveillance system, including their willingness to participate. It encompasses recognition of the system’s importance, satisfaction with processes, trust in data management, perceived reporting burden, and alignment with ethical and cultural expectations. High acceptability is supported by clear communication, demonstrated value, privacy protections, and responsiveness to user needs.\nBackground: Acceptability was identified as a foundational concept in the evaluation of public health surveillance, being identified on 34 occassions. In our review, the concept was identified as early as 1988, to reflect \"the willingness of individuals and organizations to participate in the surveillance system\". Overall, definitions have remained largely congruent with the one introduced in 1988. However, the concept has evolved through the incorporation of new elements such as trust and engagement.\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#acceptability-and-engagement",
    "href": "dictionary_documentation.html#acceptability-and-engagement",
    "title": "dictionary documentation",
    "section": "Acceptability and engagement",
    "text": "Acceptability and engagement\nID: acceptabilityAndEngagement\nSee also: acceptability; engagement; collaboration\nSynonyms\n\nExact: N/A\nBroad: participation\n\nDefinition: N/A\nDescription: Acceptability and engagement reflect the willingness to participate in and the degree of involvement with a surveillance system. They are shaped by system design (e.g., simplicity, timeliness), perceived value (e.g., importance, relevance, trust), and social context (e.g., cultural fit, stigma, privacy concerns). High levels of acceptability and engagement improve data quality, sustainability, and stakeholder collaboration but may be limited by political will, resource constraints, and communication barriers.\nBackground: Acceptability as a surveillance system attribute has evolved since its initial conceptualization. The original CDC 1988 definition focused on willingness of individuals and organizations to participate in surveillance activities. The 2001 CDC guidelines maintained this core concept while expanding factors that influence acceptability. Calba et al. (2015) broadened the concept to include not just willingness but \"the degree to which each of these users is involved in the surveillance.\" Contemporary understanding has further expanded to encompass community engagment, participation, and trust—reflecting a shift from traditional, top-down surveillance approaches to more collaborative models. This evolution acknowledges that modern surveillance systems must be acceptable to a wider range of stakeholders beyond traditional public health departments, including affected communities, healthcare providers, private sector entities, and various levels of government. The concept now implicitly recognizes cultural appropriateness, and ethical considerations as components of acceptability.\nMeasurement: Measuring acceptability often requires qualitative methods to capture user perspectives and motivations."
  },
  {
    "objectID": "dictionary_documentation.html#accessibility-data",
    "href": "dictionary_documentation.html#accessibility-data",
    "title": "dictionary documentation",
    "section": "Accessibility (data)",
    "text": "Accessibility (data)\nID: accessibilityData\nSee also: data management and storage; interoperability\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The ease with which authorized users can retrieve surveillance data and metadata using standardized protocols.\nDescription: [To incoporate FAIR in description]. Accessibility in public health surveillance refers to how readily and consistently surveillance outputs (such as alerts, reports, or predictive model results) and input data (such as raw surveillance data or supporting information) are available to end-users, decision-makers, and planners. This includes availability through standardized and sustainable protocols, user-friendly dissemination mechanisms, and manageable data formats. Accessible systems reduce technical, administrative, logistical, and practical barriers, enabling timely decision-making and effective public health responses.\nBackground: FAIR data concepts, introduced by Yang et al. in 2022, have become increasingly prominent. Definitions of accessibility commonly include providing data “when and how users need them,” although these elements overlap with system availability and communication and dissemination. Accessibility involves both output accessibility (ease of disseminating surveillance results to users) and input accessibility (consistent availability and usability of source data). Effective accessibility also requires sufficient human and technological resources, clear authorization balancing openness with privacy and security, and robust infrastructure for reliable data flow.\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#accuracy",
    "href": "dictionary_documentation.html#accuracy",
    "title": "dictionary documentation",
    "section": "Accuracy",
    "text": "Accuracy\nID: accuracy\nSee also: precision; bias\nSynonyms\n\nExact: validity, data correctness, bias (Peyre 2019, 2022), comparability\nBroad: data quality?\n\nDefinition: The proportion of data entries that correctly reflect the true value of the data collected.\nDescription: [Comparison to the reference standard, ISO,varying definitions]. Accuracy is influenced by several factors, including the sensitivity and specificity of case definitions, the clairty of surveillance forms (whether hardcopy or electronic), the technical competence and training of staff, and the care exercised in data management. It is positively correlated with several other data quality attributes/terms, including representativeness, precision, completeness, and consistency, and is negatively correlated with bias. System acceptability can influence how accurately data are captured and recorded.\nBackground:\n\nAccuracy was identified as an independent concept on eight occassions, distinct from the definitions of \"data quality\" or \"data completeness and correctness\". However, this concept has been defined inconsistently across the literature, with terminology varying by discipline - often referred to as validity, accuracy, or data correctness. The definition of accuracy that we have adopted aligns with the DQO and ISO/IEC 25012 definition.\n\n\nThe definition aligns with established international standards, including ISO 8000-2:2020, which defines accuracy as the correctness of data relative to real-world conditions. It also corresponds to definitions in the Data Quality Ontology (DQO), which emphasizes accuracy as the correctness and precision with which data represent reality. Accuracy assessment often involves validation procedures, comparisons against reference standards, or independent verification processes.\n\nMeasurement: The accuracy of a surveillance system can be accessed using historical data or simulations. To assess its effectiveness in detecting outbreaks, key metrcis should include the number of outbreaks detected, false alarms, and outbreaks missed or detected late. These assessments should be integrated into the system’s routine workflow and have the ability to be conducted with minimal effort or complexity. Routine reporting should be automated where possible. Essential data include: the number of statistical aberrations detected at a set threshold in a defined period of time (e.g., frequency per month at a given p-value); actions taken in response to signals (e.g., review for data errors, in-depth follow-up analysis of the specific conditions within the syndrome category, manual epidemiologic analysis to characterize a signal, examining data from other systems, and increasing the frequency of reporting from affected sites); resources allocated for follow-upt; public health response (e.g., an alert to clinicians, timely dissemination of information to other health entities, a vaccination campaign, or no further response); documentation of how every recognized outbreak was detected; an assessment of the value of the follow-up effort (e.g., the effort was an appropriate application of public health resources); a detailed description of the agent, host, and environmental conditions of the outbreak; and the number of outbreaks detected only late in their course or in retrospect."
  },
  {
    "objectID": "dictionary_documentation.html#adaptability",
    "href": "dictionary_documentation.html#adaptability",
    "title": "dictionary documentation",
    "section": "Adaptability",
    "text": "Adaptability\nID: adaptability\nSee also: N/A\nSynonyms\n\nExact: Flexibility\nBroad: N/A\n\nDefinition: N/A\nDescription: N/A\nBackground: –\nMeasurement: –"
  },
  {
    "objectID": "dictionary_documentation.html#adherence-suggest-change-to-standards-use-or-standards-adherence",
    "href": "dictionary_documentation.html#adherence-suggest-change-to-standards-use-or-standards-adherence",
    "title": "dictionary documentation",
    "section": "Adherence — suggest change to “Standards use” or “Standards adherence”",
    "text": "Adherence — suggest change to “Standards use” or “Standards adherence”\nID: adherence\nSee also: compliance\nSynonyms\n\nExact: N/A\nBroad: standards use\n\nDefinition: The degree to which a surveillance system follows and implements established guidelines, protocols, classification systems, and data exchange standards in its core and supporting functions.\nDescription: Standardization encompasses adherence to recognized guidelines across multiple dimensions of surveillance systems, including methodological protocols, data structures, classification systems, and exchange formats. This attribute covers both technical standards (data formats, interoperability frameworks) and operational standards (case definitions, reporting procedures). Standards use is closely related to compliance, but differs in focus. While standards use focuses on internal consistency in following best practices and operational protocols, compliance is centred around relevant legislation, regulations and policies, including ethics and security. Beyond compliance, standards use is also tied to other data quality attributes/terms, and is positively correlated with consistency ans technical competence and training.\nBackground:\n\nThe concept of adherence was identified six times in our scoping review. However, the overarching concept referred to here was only identified by Yang et al., (2022), with elements of this definition being referenced multiple times throughout the literature (e.g., standards use, transparency).\n\n\nHigh adherence is essential for the consistent functioning of the system, as it reduces errors, ensures uniformity, and facilitates the accurate tracking of health-related events or environmental changes. A system with high adherence is more likely to produce accurate and high-quality data, as following established protocols reduces errors and inconsistencies. It also enhances consistency by ensuring uniform application of procedures across time and by different personnel. Furthermore, training and supervision play a critical role in maintaining adherence, as they ensure that individuals are well-equipped to follow protocols correctly and consistently.\n\nMeasurement: Evaluating adherence involves assessing key factors such as the presence of an auditing process to ensure protocols are consistently followed, and whether data collection methods are standardized to maintain high-quality data. It also requires examining the use of harmonized indicators and metrics across sectors to ensure consistent data interpretation. Additionally, adherence can be evaluated by reviewing the use of standardized data elements (e.g., LOINC for lab tests) and electronic data interchange protocols (e.g., Health Level 7). Regular documentation of system protocols, including surveillance strategies and methodologies, is important. Additionally, verifying the enforcement of mandatory requirements for system implementation and data reporting is critical for ensureing full adherence"
  },
  {
    "objectID": "dictionary_documentation.html#availability-system",
    "href": "dictionary_documentation.html#availability-system",
    "title": "dictionary documentation",
    "section": "Availability (system)",
    "text": "Availability (system)\nID: availablity\nSee also: Accessibility (data); N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The ability to be operational when needed (Groseclose 2017; Peyre 2019, Peyre 2022; Muellner 2018; Hoinville 2013; Yang 2022; Albali 2023; Amato 2023; Anonymous 2001; Azofeifa 2018; Babaie 2015; Baker 2006; Buehler 2004, 2008; Calba 2015; Drewe 2012, 2015; Harper 2011; Innes 2022; Lucero-Obusan 2022; Marbus 2020; Meynard 2008; Ng’etich 2021; Sosin 2003; Bordier 2019; Velasova 2015).\nDescription: Availability reflects a system’s readiness to reliably collect, process, and disseminate surveillance data when required. It encompasses the continuous functioning of essential components, including infrastructure, skilled personnel, and communication channels, ensuring timely public health responses.\nBackground: –\nMeasurement: –"
  },
  {
    "objectID": "dictionary_documentation.html#benefit",
    "href": "dictionary_documentation.html#benefit",
    "title": "dictionary documentation",
    "section": "Benefit",
    "text": "Benefit\nID: benefit\nSee also: costs; impact\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The direct and indirect advantages produced by the surveillance system.\nDescription: [Carol’s + time horizon component]. The benefit term is influenced by several factors, including the scope of surveillance objectives, the effectiveness of data use in decision-making, and the capacity for early detection and response. It encompasses direct and indirect outcomes such as cost savings, improved public and animal health, enhanced trade, and prevention of losses in productivity, morbidity, and mortality. Benefits are positively correlated with terms such as timeliness, usefulness, and stability, and are shaped by stakeholder engagement and collaboration. The distribution of benefits across sectors also informs the overall value of the surveillance system.\nBackground: Benefit was identified as an independent concept nine times in our scoping review, distinct from related concepts such as costs, impact, and efficiency. The concept was first introduced by Hoinville et al., in 2013 and was consistently included in animal health surveillance system evaluation frameworks thereafter. The definition has remained largely unchanged with the execption of Calba et al., 2015 introducing the idea of assessing whether users are satisfied that their requirements have been met in their definition. Their paper also focused solely on non-monetary benefits. However, despite the alignment of this definition across animal health surveillance evaluation frameworks, benefit was not identified as a standalone concept in the context of public health surveillance.\nMeasurement: The evaluation methods for the benefits of surveillance activities involve identifying and quantifying both direct and indirect benefits. One approach includes contingent valuation methods (CVM) like proportional piling, where stakeholders are asked what they would be willing to pay for improvements such as sanitary information. The evaluation process should list all potential benefits, categorize them, and quantify market benefits where possible. Non-monetary benefits, such as improved public health or consumer confidence, should also be considered using alternative methods like quality-adjusted life years (QALY). Additionally, the distribution of benefits across various stakeholders, such as producers, consumers, and the livestock industry, should be assessed. The relationship between surveillance, intervention, and the mitigation of losses should be explored, as many benefits arise from integrated disease control. Finally, the evaluation should capture indirect benefits like enhanced trade opportunities and the improved capacity to respond to future threats or outbreaks."
  },
  {
    "objectID": "dictionary_documentation.html#bias-ascertainment-performance",
    "href": "dictionary_documentation.html#bias-ascertainment-performance",
    "title": "dictionary documentation",
    "section": "Bias (ascertainment performance)",
    "text": "Bias (ascertainment performance)\nID: biasAscertainmentPerformance\nSee also: accuracy; precision; sensitivity; specificity; representativeness; data quality; predictive value positive; false detection rate\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The extent to which a prevalence estimate produced by the surveillance system deviates from the true prevalence value.\nDescription: Case detection bias occurs when systematic measurement errors cause surveillance data to systematically differ from the true occurrence of the health event in the target population. Bias can arise from multiple sources including diagnostic test characteristics, reporting practices, population coverage, sampling methods, case definitions, and temporal factors. The magnitude and impact of bias depends on surveillance objectives. For general population surveillance, bias can distort prevalence estimates and trend analysis, leading to incorrect public health conclusions.\nBackground:\n\nBias was identified as a reported concept six times in our scoping review. The concept was first introduced by Hoinville et al., in 2013 and was consistently included in animal health surveillance system evaluation frameworks thereafter. The definition has remained largely unchanged with the execption of Peyre 2019 & 2022 incorporating the idea of \"bias=accuracy\" in their definition. Despite its frequent application in evaluating animal health surveillance systems, the term \"bias\" was not identified in the context of public health surveillance.\n\n\nEvaluating bias in surveillance systems is essential for accurately monitoring endemic diseases.\n\nMeasurement: Evaluating case detection bias (also called ascertainment bias) involves assessing systematic errors that affect a surveillance system’s ability to accurately identify and classify cases or events. This includes selection bias (when certain cases or populations are systematically over- or under-represented) and information bias (when data collection methods systematically affect measurement accuracy). Bias represents the consequence of measurement errors that can be characterized through surveillance performance measures such as sensitivity, specificity, and representativeness. Evaluating bias involves assessing potential sources, such as diagnostic sensitivity and specificity, underreporting in passive surveillance, and how the sample population is chosen (e.g., animals from abattoirs might not represent the overall population). Additionally, geographic, demographic, or ecological factors can contribute to bias. To quantify and correct for bias, methods like comparison across multiple data sources, simulation models, statistical methods, or capture-recapture analyses can be used. Assessment of bias (systematic error) should address the following: * Selection bias: systematic differences between sample and target populations * Performance bias: systematic differences between groups in care provided or in exposure to factors other than the interventions of interest * Detection bias: systematic differences in how the outcome is determined (eg, death scene investigation protocols) * Attrition bias: systematic loss to follow up * Reporting bias: systematic differences in how people report symptoms or ideation * Other: biases related to a particular data source"
  },
  {
    "objectID": "dictionary_documentation.html#case-definition",
    "href": "dictionary_documentation.html#case-definition",
    "title": "dictionary documentation",
    "section": "Case definition",
    "text": "Case definition\nID: caseDefinition\nSee also: bias (case detection)\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The standard set of criteria used to identify and classify cases or events for consistent reporting and analyses within a surveillance system.\nDescription: The case definition affects surveillance system performance by determining how accurately and consistently cases are identified and classified. A well-designed case definition supports accurate and consistent detection, improving the quality of the data collected. It directly influences terms like sensitivity, specificity, representativeness, and coverage, and plays a role in determining the system’s cost, acceptability, and flexibility. Clear defined definition may also improve consistency across sites and over time.\nBackground: The concept of case definition was identifed five times throughout our scoping review.\nMeasurement: Qualitative assessment methods have been used to evaluate this concept, such as the use of the EVARISK tool."
  },
  {
    "objectID": "dictionary_documentation.html#collaboration",
    "href": "dictionary_documentation.html#collaboration",
    "title": "dictionary documentation",
    "section": "Collaboration",
    "text": "Collaboration\nID: collaboration\nSee also: acceptability and engagement; governance\nSynonyms\n\nExact: partnerships\nBroad: N/A\n\nDefinition: The active engagement and coordination among diverse partners and end-users to facilitate exchange of data, information, and knowledge, sharing of capacities, and collaborative implementation of surveillance activities.\nDescription: Collaboration in surveillance systems relies on clear roles, strong legal frameworks, and established inter-sectoral mechanisms. It occurs in four main areas: governance across sectors (health, animal, environment), decision-making at all levels (local to international), interdisciplinary efforts (biosciences, social sciences, engineering), and public-private partnerships (e.g., veterinary companies in antimicrobial resistance). Effective data exchange and a shared databases enhance collaboration. [still working on this one somewhat since multiple terms are being folded into this].\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#communication-and-dissemination",
    "href": "dictionary_documentation.html#communication-and-dissemination",
    "title": "dictionary documentation",
    "section": "Communication and dissemination",
    "text": "Communication and dissemination\nID: communicationAndDissemination\nSee also: collaboration\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The ability of the system to deliver information and data in a clear and distinct manner to relevant stakeholders inside and outside of the surveillance system.\nDescription: Effective communication and dissemination in surveillance systems involve sharing clear, relevant, and timely information with all stakeholders, actors, and end-users (both internally and externally). Surveillance data outputs must meet the needs of diverse users, including data providers, analysts, decision-makers, and the public. Outputs should be accessible, frequent, and based on up-to-date, high-quality data with clear explanations of limitations and biases. Communication and disseminationis is also closely linked to acceptability, timeliness, impact, and collaboration.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#compatability",
    "href": "dictionary_documentation.html#compatability",
    "title": "dictionary documentation",
    "section": "Compatability",
    "text": "Compatability\nID: compatibility\nSee also: Coherence; interoperability\nSynonyms\n\nExact: N/A\nBroad: Integration\n\nDefinition: Compatibility with and ability to integrate data from other sources and surveillance components, e.g., one health surveillance (part of data collection and data management) (Peyre 2019, Peyre 2022).\nDescription: N/A\nBackground: –\nMeasurement: –"
  },
  {
    "objectID": "dictionary_documentation.html#compliance",
    "href": "dictionary_documentation.html#compliance",
    "title": "dictionary documentation",
    "section": "Compliance",
    "text": "Compliance\nID: compliance\nSee also: adherence\nSynonyms\n\nExact: N/A\nBroad: legislative support\n\nDefinition: The degree to which the surveillance system complies with all relevant legislation, regulations and policies, including ethics and confidentiality requirements.\nDescription: Compliance refers to meeting required legal or regulatory requirements, often with external oversight. It is closely related to adherence, but differs in focus. While compliance focuses on relevant legislation, regulations and policies, including ethics and security, adherence is centred around internal consistency in following best practices and operational protocols. Beyond adherence, compliance is also tied to other data quality attributes/terms, and is positively correlated with consistency ans technical competence and training.\nBackground: The concept of compliance was identified twice in our scoping review. However, the overarching concept referred to here was only identified by Yang et al., (2022), with components of this definition being referenced multiple times throughout the literature (e.g., legislative support, security, ethical considerations).\nMeasurement: To evaluate compliance, system security policies and practices should be reviewed to ensure that security levels and procedures for surveillance system data or system access are defined and enforced. Data use and release policies and protocol should be available for review, and access to the surveillance system software applications should be controlled. [Need to add information about legislative support/policies and ethics]."
  },
  {
    "objectID": "dictionary_documentation.html#consistency",
    "href": "dictionary_documentation.html#consistency",
    "title": "dictionary documentation",
    "section": "Consistency",
    "text": "Consistency\nID: consistency\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: repeatability, precision\n\nDefinition: Data have the same meanings (e.g. case definition, diagnosis standards) to allow for consistent interpretation. This includes internal consistency (within a dataset) and external consistency (across different data sources), as well as consistency over time.\nDescription: Consistency refers to the uniformity and reliability of data across time and sources within the surveillance system. It ensures that data remain unchanged when replicated, transferred, or integrated between partners. Consistent data collection methods, coding systems, case definitions, and documentation of transformations are essential for comparability and trend analysis.\nBackground: Consistency was identified five times in our scoping review.\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#cost-benefit",
    "href": "dictionary_documentation.html#cost-benefit",
    "title": "dictionary documentation",
    "section": "Cost-benefit",
    "text": "Cost-benefit\nID: costBenefit\nSee also: cost minimization; costs; cost-effectiveness; cost-utility; benefit\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: An analysis where all costs and outcomes are expressed in monetary terms, calculating the net gain or loss by measuring both the costs of an intervention and the monetary value of its outcomes.\nDescription: Cost-benefit analysis quantifies both the costs and outcomes of a surveillance system in monetary terms to assess its net economic value. This allows comparison of investments against financial benefits such as reduced disease burden, healthcare savings, or productivity gains. It supports decision-making by highlighting the economic return of surveillance activities relative to their expense. Accurate valuation of outcomes and comprehensive cost accounting—including direct, indirect, fixed, and variable costs—are essential. This term complements cost-effectiveness and cost-minimization analyses by providing a broader economic perspective on surveillance system value.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#cost-effectiveness",
    "href": "dictionary_documentation.html#cost-effectiveness",
    "title": "dictionary documentation",
    "section": "Cost-effectiveness",
    "text": "Cost-effectiveness\nID: costEffectiveness\nSee also: cost minimization; cost-benefit; costs; cost-utility; benefit\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: Relationship between the expected outcomes and the costs of surveillance to achieve these outcomes. Surveillance system costs include direct costs (personnel and material resources), indirect costs (resulting from preparedness and response to surveillance findings), and prevention benefits or costs from a societal perspective (e.g., effects of the information generated on decision making and population health). Refers to the relative expenditures (costs) and outcomes (effects) of different surveillance strategies, determining whether the improved health outcomes or surveillance capabilities justify the associated costs to help inform efficient resource allocation decisions (kirsh 2008 adpated for surveillance).\nDescription: The costs of obtaining surveillance information needs to be balanced against the benefits derived. Assessment of surveillance resources typically focuses on direct costs. Because of the complexity of surveillance and response processes, it is usually difficult to define indirect costs. For some infectious disease surveillance systems, investigators have modeled the expected future costs of strategies for continued vaccination, surveillance, and other public health activities. Efforts to improve sensitivity, positive predictive value, representativeness, timeliness, and stability can increase the cost of a surveillance system.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#cost-minimization",
    "href": "dictionary_documentation.html#cost-minimization",
    "title": "dictionary documentation",
    "section": "Cost minimization",
    "text": "Cost minimization\nID: costMinimization\nSee also: costs; cost-benefit; cost-effectiveness; cost-utility; benefit\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The comparison of costs between alternatives with identical outcomes, used to determine the most economical option when no additional benefits are present (often applied to demonstrate the dominance of one strategy over another).\nDescription: Cost-minimization compares the expenses of alternative surveillance strategies that achieve identical outcomes to identify the most economical option. It helps prioritize resource allocation by demonstrating which approach delivers the same public health impact at the lowest cost. This term is closely linked to cost-effectiveness and efficiency, but unlike those, it assumes equivalent benefits across options. Accurate cost data—including fixed and variable costs—are essential. Considering operational, personnel, and infrastructure expenses ensures a comprehensive evaluation, supporting sustainable and affordable surveillance system design without compromising performance.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#cost-utility",
    "href": "dictionary_documentation.html#cost-utility",
    "title": "dictionary documentation",
    "section": "Cost-utility",
    "text": "Cost-utility\nID: costUtility\nSee also: cost minimization; cost-benefit; cost-effectiveness; costs; benefit\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: An evaluation that compares the system’s interventions or strategies by measuring outcomes in terms of utility gained, using a ratio scale to assess the value of different health states or surveillance outcomes, allowing for comparison across various surveillance approaches and their impact on public health.\nDescription: Cost-utility analysis compares surveillance strategies by measuring outcomes in terms of utility, often using quality-adjusted life years (QALYs) or disability-adjusted life years (DALYs). This allows valuation of health benefits considering both quantity and quality of life gained from interventions. It facilitates comparison across diverse surveillance approaches with different health impacts, integrating effectiveness and economic costs into a single metric. Accurate utility measurement requires reliable health state valuation and comprehensive cost data. This term complements cost-benefit and cost-effectiveness analyses by focusing on health-related quality of life, supporting informed resource allocation in public health surveillance.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#costs",
    "href": "dictionary_documentation.html#costs",
    "title": "dictionary documentation",
    "section": "Costs",
    "text": "Costs\nID: costs\nSee also: cost minimization; cost-benefit; cost-effectiveness; cost-utility; benefit\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: All expenses associated with the surveillance system, including direct costs (e.g., equipment, personnel) and indirect costs (e.g., training, data management infrastructure).\nDescription: Cost is a key consideration in evaluating surveillance systems, reflecting their value and efficiency. Costs are both direct (e.g., personnel, data collection, laboratory services, infrastructure) and indirect (e.g., follow-up efforts for false alarms, impact on agency credibility). New or improved systems incur start-up (e.g., software, equipment) and ongoing operational costs (e.g., staff, maintenance). Economic analyses should balance these costs with the public health benefits, such as early outbreak detection and disease prevention, to assess overall value. Direct and indirect costs are usually collected and reported separately.\nBackground: TBD\nMeasurement: To evaluate costs, all elements of the surveillance system must be considered, including planning, sample collection, analysis, reporting, and dissemination. It’s important to distinguish between fixed costs (e.g., permanent staff salaries) and variable costs (e.g., sample collection or reagents). Cost-sharing among stakeholders—such as producers, consumers, and public funds—should also be considered, as these costs can vary across the system."
  },
  {
    "objectID": "dictionary_documentation.html#coverage",
    "href": "dictionary_documentation.html#coverage",
    "title": "dictionary documentation",
    "section": "Coverage",
    "text": "Coverage\nID: coverage\nSee also: representativeness\nSynonyms\n\nExact: N/A\nBroad: representativeness?\n\nDefinition: Proportion of the population of interest (target population) or proportion of areas of interest (e.g. specific habitats or high- risk sites) that is included in the surveillance activity.\nDescription: Coverage indicates the extent to which the target population or geographic areas are included in surveillance. High coverage enhances representativeness, sensitivity, and early detection—especially for emerging threats. It is related to sampling strategy, reporting completeness, and access to high-risk populations. Mathematically, coverage is the proportion the same represents divided by the total target population size.\nBackground: Coverage was identified as a reported concept six times in our scoping review. The concept was first introduced by Hoinville et al., in 2013 and has since been consistently included in animal health surveillance system evaluation frameworks. The definition has remained highly consistent within animal health surveillance, aligning with the concept of \"population coverage\" in public health surveillance.\nMeasurement: Coverage can be assessed by the proportion of respondents (survey-based) or cases (hospital- or facility-based) included in the surveillance system. Key measurements include population undercoverage, which occurs when parts of the target population are missed, and population overcoverage, where elements outside the target population are included. A demographic analysis can provide benchmarks for assessing completeness of coverage in the existing surveillance data and document changes in coverage from previous periods. Additionally, systems like farmer-based clinical surveillance, syndromic surveillance, or periodic sampling surveys benefit from regular evaluations to identify gaps or misrepresentation in coverage. By reviewing the geographic coverage and the number of areas of interest, it is easier to determine if the surveillance system adequately represents the target population. Furthermore, temporal coverage can be assess by determining the conditional probability that any given unit in the population will be examined or tested within the specified time frame. For example, if the target time frame is 7 days, but testing occurs every 4 weeks, the temporal coverage would be 25%. An assessment of coverage could include: * Characterisation and qualitative comparison of the sampled and target populations. Alternatively, comparison of sampled areas or habitats versus areas or habitats of interest. * Where sufficient data on the target population/areas or habitats of interest exists, simple calculations of the proportional coverage can be made (e.g. 75% of the national herd and 45% of cattle holdings are sampled annually or 30% of the marine ports). * Where sufficient information on the background population, or high-risk areas or habitats respectively, is lacking, more sophisticated sampling designs might be employed (e.g. capture-recapture analysis or drop camera surveillance). * Considering whether the target population, or area of interest, has been adequately defined (i.e. whether the exclusion of certain animals or holdings or sites is merited). * The unit of interest - the level at which coverage is measured - is often the unit of interest of surveillance (e.g. animal, holding, high-risk site or specific marine habitat). If insufficient data exist, alternative perspectives might be desired. Coverage might then be assessed at other aggregate levels (e.g. geographical areas) or relevant intermediate steps in the surveillance pathway (e.g. the proportion of veterinary practices submitting diagnostic samples). * In certain contexts it may be worth establishing a timeframe of reference (e.g. annual coverage). The choice of timeframe should reflect the epidemiology of the disease or life history of a risk organism."
  },
  {
    "objectID": "dictionary_documentation.html#credibility-collaboration",
    "href": "dictionary_documentation.html#credibility-collaboration",
    "title": "dictionary documentation",
    "section": "Credibility (collaboration)",
    "text": "Credibility (collaboration)\nID: credibilityCollaboration\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: N/A\nDescription: TBD\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#credibility-data",
    "href": "dictionary_documentation.html#credibility-data",
    "title": "dictionary documentation",
    "section": "Credibility (data)",
    "text": "Credibility (data)\nID: credibilityData\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: N/A\nDescription: TBD\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#data-accessibility",
    "href": "dictionary_documentation.html#data-accessibility",
    "title": "dictionary documentation",
    "section": "Data accessibility",
    "text": "Data accessibility\nID: dataAccessibility\nSee also: N/A\nSynonyms\n\nExact: Accessibility (data)\nBroad: –\n\nDefinition: –\nDescription: N/A\nBackground: –\nMeasurement: –"
  },
  {
    "objectID": "dictionary_documentation.html#data-analysis",
    "href": "dictionary_documentation.html#data-analysis",
    "title": "dictionary documentation",
    "section": "Data analysis",
    "text": "Data analysis\nID: dataAnalysis\nSee also: data collection; data management and storage\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The use of appropriate methods for the analysis and interpretation of data.\nDescription: &lt;+ Data analysis applies scientific methods to process surveillance data to generate insights such as detecting unusual patterns (i.e. outbreaks, signal levels beyond expected). Methods range from basic descriptive statistics and trend monitoring to more sophisticated approaches like time series, spatial analysis, and automated aberration detection algorithms. +&gt; assesses how effectively surveillance data are processed and interpreted to generate insights. High-performing systems apply appropriate, context-specific analytical methods and ensure findings are actionable.\nBackground: Data analysis was cited as a concept in surveillance system evaluation nine times in the literature, spanning public health, animal health, and One Health surveillance system evaluation frameworks. The definition remains largely consistent, &lt;+ relfecting uniformity across disciplines, with refinment over time, to emphasize the use of contextual information (e.g., prior likelihood of disease) in risk-based analysis and the need for analysis/interpretation to be performed timely to support decision-making and action. +&gt;\nMeasurement: The evaluation of the data analysis attribute/term focuses on assessing the methods used to analyze surveillance data. It includes identifying the techniques applied, such as basic statistics, trend analysis, or more advanced methods like time series and spatial analyses. The evaluation checks if the limitations of the data have been properly considered and addressed in the analysis. Additionally, it evaluates whether the data is being fully utilized or if there are opportunities for further analysis. Reviewing past user needs and whether the analysis methods have met these demands is also important to ensure the analysis is delivering valuable, actionable insights. An evaluation of data analysis should include: The identification of the analysis methods applied to surveillance data: * No analysis * Basic descriptive statistics * Examination of trends * More sophisticated statistical approaches (e.g. time series analyses, spatial analyses); An assessment of whether the limitations of data have been understood and accounted for in statistical analyses?; An indication as to whether the body of data available is being fully exploited or could further use of data be made? It may help to review requirements for information made by users of the surveillance data in the past, to determine whether their needs were met by the methods applied."
  },
  {
    "objectID": "dictionary_documentation.html#data-collection",
    "href": "dictionary_documentation.html#data-collection",
    "title": "dictionary documentation",
    "section": "Data collection",
    "text": "Data collection\nID: dataCollection\nSee also: data analysis; data management and storage\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The use of appropriate data sources and collection methods, including a clear a case definition and a data collection protocol.\nDescription: Effective data collection relies on clearly defined and complete case definitions or risk organism descriptions, including case signalment, clinical and pathological signs, and epidemiological information, and, where relevant, laboratory or taxonomic identification. Diagnostic methods must align with the case definition and be assessed for sensitivity and specificity. A written protocol should guide data and sample collection, with mechanisms to ensure adherence. Data collection methods should be clearly documented, and collection methods should minimize redundancy, address data gaps, and employ appropriate sampling strategies (e.g., risk-based or pooled).\nBackground: Data collection is a fundamental surveillance activity consistently recognized in evaluation frameworks across public, animal, and One Health systems (five instances within the literature review). Data collection was explicitly mentioned as a concept in the evaluation of surveillance systems in five instances within the literature, including a clear definition and integration into the animal health surveillance field. It is commonly cited as the most costly and difficult component of a surveillance system to implement (Chaudron, 2010; Peyre et al. 2022).(1)\nMeasurement: Questions to consider when assessing data and information collection include: * Is there (if applicable) a written case definition/organism description for this surveillance system that is clearly defined and complete, with specified inclusion and exclusion criteria? If so Does the case definition/organism description include relevant details of the case signalment, clinical and pathological signs and epidemiological information as appropriate? Does the case definition include laboratory diagnosis? Alternatively does the organism description include taxonomic ID? Are the chosen diagnostic/taxonomic methods appropriate to the case definition/organism description, including in terms of samples being collected? Are syndromes used and - if yes - defined in an appropriate way? In those sectors where symptomatic surveillance is * Is there (if applicable) a written case definition/organism description for this surveillance system that is clearly defined and complete, with specified inclusion and exclusion criteria? If so undertaken, are symptoms clearly defined for when samples need to be taken? * Is there a written sample and/or data collection protocol and are there appropriate assurance mechanisms to ensure the protocols are followed? * Have the sensitivity and specificity of the tests used been assessed (where relevant)? * Are there data collected that are not used in analysis, interpretation or surveillance management (redundancy)? * Are there information needs for which data are not currently collected and feasibly could be? * Are appropriate sampling strategies used, including the use of risk-based approaches and pooled sampling? This could include risk-based requirement calculations or risk- based sampling. The basis of the risks used in the design of the risk-based sampling strategy should be reviewed. Data collection methods should be clearly documented. It may help to review demands for information made by users of the surveillance data in the past, to determine whether their needs were met by the data available."
  },
  {
    "objectID": "dictionary_documentation.html#data-completeness",
    "href": "dictionary_documentation.html#data-completeness",
    "title": "dictionary documentation",
    "section": "Data completeness",
    "text": "Data completeness\nID: dataCompleteness\nSee also: coverage; accuracy\nSynonyms\n\nExact: data completeness and correctness (completeness)\nBroad: N/A\n\nDefinition: The proportion of data that were intended to be collected that actually was collected.\nDescription: Completeness refers to the extent to which all required data are captured, reported, and available for analysis. A system with high completeness allows for more reliable decision-making and enhances the system’s usefulness. Incomplete data may result from issues with data collection, management, or communication, undermining the system’s effectiveness. Regular submission from all contributors and strong data management practices help improve completeness. Ensuring completeness strengthens data quality.\nBackground: Completeness was identified as an independent concept on 12 occassions, distinct from the definitions of \"data quality\" or \"data completeness and correctness\". The concept was first introduced by Thacker et al., in 1988 (A method for evaluating systems of epidemiological surveillance), however it was not indentified as an evaluation concept until 1993 by Cutts et al. The definition of completeness has remained largely unchanged over the years, maintaining consistency across disciplines. It is primarily regarded as a key aspect of data quality and continues to be one of the most frequently used concepts for evaluating surveillance systems.\nMeasurement: An evaluation of completeness should involve assessing the proportion of cases that were identified and subsequently reported to the system, which can be done through register/database reviews. Additionally, examining the percentage of “unknown” or “blank” responses to items on surveillance forms offers a straightforward and effective measure. The evaluation should also address challenges faced in both manual and automated data management. This includes identifying issues such as coding errors or data loss in manual systems, and programming errors or inappropriate data filtering in automated systems. These evaluations help identify gaps in data reporting and ensure that the surveillance system captures all relevant cases accurately."
  },
  {
    "objectID": "dictionary_documentation.html#data-correctness",
    "href": "dictionary_documentation.html#data-correctness",
    "title": "dictionary documentation",
    "section": "Data correctness",
    "text": "Data correctness\nID: dataCorrectness\nSee also: N/A\nSynonyms\n\nExact: accuracy (correctness), completeness, data quality\nBroad: N/A\n\nDefinition: The proportion of data entries that correctly reflect the true value of the data collected.\nDescription: N/A - data completeness and correctness is an exact synonym for data quality.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#data-management",
    "href": "dictionary_documentation.html#data-management",
    "title": "dictionary documentation",
    "section": "Data management",
    "text": "Data management\nID: dataManagement\nSee also: N/A\nSynonyms\n\nExact: –\nBroad: –\n\nDefinition: –\nDescription: –\nBackground: –\nMeasurement: –"
  },
  {
    "objectID": "dictionary_documentation.html#data-management-and-storage",
    "href": "dictionary_documentation.html#data-management-and-storage",
    "title": "dictionary documentation",
    "section": "Data management and storage",
    "text": "Data management and storage\nID: dataManagementAndStorage\nSee also: historical data\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: Appropriate use and documentation of data management systems for processing information, including data processing protocols, and effective use of data verification procedures and of data storage and back-up procedures.\nDescription:\n\nAppropriate use and documentation of data management systems for processing information, including data processing protocols and effective use of data verification procedures, data storage and back-up protocols. Measures taken to assure authorised computer system access and to maintain confidentiality where needed. Is there a dedicated custodian? Data management is a broad area concerning the collation, storage and maintenance of data, including but not limited to matters of data quality, accessibility, usefulness and security. Assessing this attribute will require an intimate understanding of the data storage and management systems employed by the surveillance activity.\n\n\nData management is a broad area concerning the collation, storage and maintenance of data, including but not limited to matters of data quality, accessibility, usefulness and security. Assessing this attribute will require an intimate understanding of the data systems employed by the surveillance activity.\n\n\nData management and storage involve the systematic handling, processing, and safeguarding of data throughout its lifecycle. This includes ensuring data quality, accessibility, and security. Effective data management requires robust protocols for processing, verification, storage, and backup, as well as measures to control access and maintain confidentiality where necessary. An essential consideration is the appointment of a dedicated custodian responsible for overseeing these activities.\n\nBackground: TBD\nMeasurement: Evaluating data management and storage involves reviewing the database structure to verify that data fields are well-defined, consistent, and correctly normalized, ensuring efficient storage and retrieval. Evaluators should check that primary keys uniquely identify records and that validation constraints and cross-consistency checks are in place to prevent invalid data input. The documentation supporting the database, such as data dictionaries and entity relationship diagrams, should be reviewed to confirm that it facilitates clear interpretation and understanding of the data. Additionally, the evaluation should assess the existence and sufficiency of data management protocols, including data quality standards (e.g., ISO9000, Good Clinical Practice). This involves reviewing records management policies, retention protocols, and periodic quality control checks. Data security and access control protocols must also be evaluated, ensuring only authorized personnel can access sensitive data.Furthermore, back-up and storage procedures should be tested to confirm data is securely stored and can be recovered if needed. Interviews with data custodians or managers may provide insights into the system’s functionality and reliability. Overall, the evaluation will require a comprehensive review of the system’s design, documentation, and operational practices to ensure proper data handling and storage. An assessment of this attribute should include: 1.Consideration of whether the database structure has been correctly designed: Has each field of data been tightly defined to ensure correctness, conciseness and consistency across records? Have primary keys, uniquely identifying each record, been assigned? Has the database been normalised, to ensure data is stored in the most parsimonious, transparent and useable way? Have validation constraints, preventing the input of invalid data, and internal cross-consistency checks been applied? Is the data stored in a way that allows the required interrogation and analysis?; 2.Consideration of whether documentation of the data is sufficient to facilitate interpretation and understanding of the data: Is there a document providing a summary overview of the data and collection methods and explaining any idiosyncrasies relevant to the analysis and interpretation of the data? Is there a data dictionary that clearly defines each field? Is there an entity relationship diagram that explains how the data relate?; 3.Consideration of whether there are adequate and documented protocols for managing data quality and security: Is the data management system covered by a data quality standard (e.g. ISO9000, Good Clinical Practice or Good Laboratory Practice)?  Are periodic data quality control checks implemented? Are records management issues clearly defined, including policy on the retention of data?  Are data back-up and storage protocols in place?"
  },
  {
    "objectID": "dictionary_documentation.html#data-quality",
    "href": "dictionary_documentation.html#data-quality",
    "title": "dictionary documentation",
    "section": "Data quality",
    "text": "Data quality\nID: dataQuality\nSee also: N/A\nSynonyms\n\nExact: data completeness and correctness\nBroad: N/A\n\nDefinition: The completeness and validity of the data recorded in the public health surveillance system.\nDescription: &lt;+ Data quality measures the inherent correctness and sufficiency of the data gathered by the surveillance system. +&gt; Data quality reflects the completeness and validity of recorded data and is essential for accurate surveillance outputs. &lt;+ Poor data quality can undermine reliability, introduce bias that limits representativeness, and make the system less acceptable to participants. Conversely, high-quality data are essential for accurate analysis and improving the system’s overall usefulness for decision-making and public health action. +&gt; High-quality data improve system acceptability and representativeness, while Poor-quality data—often due to manual entry errors in paper-based systems—can undermine reliability and may indicate problems in data collection or data management and storage. Electronic systems can enhance data quality by improving accuracy, timeliness, and accessibility. Data quality is linked to terms such as timeliness, acceptability, representativeness, and usefulness, and directly affects the reliability and usability of system outputs for decision-making and public health response.\nBackground:\n\nData quality has historically been defined by the core attributes of completeness and validity. It is routinely recognized as essential across public health, animal health, and emerging One Health surveillance frameworks. In the context of animal health, the term \"data completeness and correctness\" is used as an exact synonym for data quality.\n\n\nIn more recent years, data quality includes the concept of fit-for-purpose, with sufficeint quality to meet the need of the data’s intended purpose. As surveillance systems evolve, particularly through the use of informatics and electronic systems, data quality is enhanced and expanded. Electronic systems can improve data quality by facilitating accuracy, timeliness, accessibility, and reducing the potential for manual entry errors. As data systems evolve, data quality has been enhanced and expanded with the use of robust data management systems and automated data flow processes. The concept is highly interrelated with other performance metrics; for instance, poor quality data reduces system acceptability and representativeness.\n\nMeasurement: Can be measured as the proportion of data intended to be collected that was actually collected (completeness) and the proportion of data entries that correctly reflect the true value of the data collected (validity/accuracy). Includes proportion of unknown, invalid, and missing values for reported data elements. Validity may be estimated by the proportion of errors in surveillance system data compared to analogous data from one of the system’s data sources. Measures for determining data quality include percentages of \"unknown,\" invalid, and missing responses to items on data collection forms. In addition, data quality can be measured by applying edits for consistency in the data. However, a full assessment may require a special study. The acceptability and representativeness of a public health surveillance system are related to data quality. With data of high quality, the system can be accepted by those who participate in it. In addition, the system can accurately represent the health-related event under surveillance. Paper-based systems, such as the E-Book registry, are more prone to human error, and can result in illegible, missing, and incomplete entries. An electronic system could increase data quality. Moreover, conversion to an electronic system would improve the timeliness, accessibility, and usability of captured data, as well as allow greater flexibility in responding to health events in a community. Many data-quality definitions depend on other system performance attributes (eg, timeliness, usefulness, acceptability) (21). Because of reliance on multiple data sources, data quality must be assessed in different ways. For surveillance relying on surveys, concepts of reliability, validity, and comparison with alternative data sources are important. For example, considerations of possible data-quality concerns arise with use of mortality data, particularly underreporting of suicide. It should be assessed using measures of completeness, consistency, and comparison with alternative sources to ensure reliability across diverse data types and collection methods."
  },
  {
    "objectID": "dictionary_documentation.html#detection-mode",
    "href": "dictionary_documentation.html#detection-mode",
    "title": "dictionary documentation",
    "section": "Detection mode",
    "text": "Detection mode\nID: detectionMode\nSee also: Sensitivity; Specificity; Predictive Value Positive; False-alarm rate; Timeliness; Data analysis\nSynonyms\n\nExact: N/A\nBroad: event detection, early detection/warning, surveillance sensitivity (outbreak detection), signal detection, case detection capability\n\nDefinition: The operational capability of the surveillance system to identify or forecast an event (e.g., event detection, establish baseline, or event detection expected with no baseline).(Corley_AEC_2012?)\nDescription: Detection mode characterizes how a surveillance system operationally identifies health events and determines whether detected events represent deviations from an established baseline in terms of frequency and/or magnitude (Corley_AEC_2012?). Systems may be configured to detect events against an established baseline, to first establish baseline patterns before detection can occur, or to detect events when no baseline is available (such as for exotic or novel diseases). Any excess or anomalous occurrence identified through the system’s detection mode should be statistically characterized and include a measure of abnormality or confidence (Corley_AEC_2012?). It should be recognized that baselines fluctuate over time, requiring appropriate statistical methods for pattern recognition and aberration detection (Buehler_ESSEDa_2004?,Groseclose_ESS_2013?). The chosen detection mode directly influences how system attributes such as sensitivity (probability of detection) and timeliness (speed of detection) are defined and measured, and ultimately determines the system’s utility for public health intervention (2,Morbey_OJP_2021?,Corley_AEC_2012?).\nBackground: Detection mode provides a foundational characterization of the operational capability of a surveillance system, specifically concerning its approach to early warning and event detection (Corley_AEC_2012?). The attribute was identified and defined within the context of evaluating event-based biosurveillance systems, models, and constructs, which frequently rely on analyzing qualitative or quantitative variables to infer population health status (Corley_AEC_2012?). Within biosurveillance evaluation frameworks, detection mode falls under the broader Event Attribute Family and describes the type of operational outcome anticipated from the system (Corley_AEC_2012?). While detection mode represents a distinct operational design characteristic, it overlaps considerably with classic case detection and ascertainment metrics. The performance of a surveillance system’s detection mode is fundamentally measured through traditional epidemiological attributes including sensitivity (the probability and proportion of true events detected), specificity (the ability to correctly identify non-events), predictive value positive (the proportion of detected events that are true events), and timeliness (the speed of detection) (2,Morbey_OJP_2021?,Corley_AEC_2012?,Buehler_ESSEDa_2004?). This attribute is closely connected to syndromic surveillance methods, which heavily rely on statistical tools for pattern recognition and aberration detection applied to data streams to screen for patterns requiring investigation (Buehler_ESSEDa_2004?,Groseclose_ESS_2013?). The chosen detection mode directly determines how these classic ascertainment metrics are operationalized, defined, and measured within the surveillance system context, while balancing detection capability against false-alarm rates to ensure efficient resource utilization (3,Corley_AEC_2012?).\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#effectiveness",
    "href": "dictionary_documentation.html#effectiveness",
    "title": "dictionary documentation",
    "section": "Effectiveness",
    "text": "Effectiveness\nID: effectiveness\nSee also: usefulness\nSynonyms\n\nExact: efficacy\nBroad: Impact, usefulness\n\nDefinition: The extent to which a surveillance system is able to achieve its intended objectives. (4,Drewe_EoAaPHS_2012?,WorldHealthOrganization_OtWF_2004?,Peyre_TRVA_2019?).\nDescription: Effectiveness reflects how well the surveillance system achieves its intended objectives and meets the needs of stakeholders. It represents the capability of producing a desired result and encompasses the system’s technical performance in reaching its objectives, such as disease control or early detection (4,Drewe_EoAaPHS_2012?). The measurement of effectiveness considers multiple underlying attributes including timeliness, sensitivity, specificity, predictive value, and robustness, with the specific attributes of greatest relevance determined by the surveillance system’s objectives (Grosbois_ARtUMoE_2015?,Yang_UOaHS_2022?). Sensitivity (probability of detection) and timeliness are frequently assessed attributes related to effectiveness, particularly for systems focused on early detection. Systems should use collaborative approaches to improve effectiveness across data collection, processing, and dissemination, as integrated surveillance can enhance effectiveness compared to systems operating in isolation (Yang_UOaHS_2022?). Effectiveness is evaluated through indicators that measure processes, outputs, outcomes, and impacts against stated objectives. These indicators may include task completion rates, timeliness metrics, and measurable public health impact, such as whether the system leads to prevention, control, or better understanding of adverse health events (Muellner_SiFAiB_2018?,Peyre_TRVA_2019?). This term is closely related to system usefulness (the tangible benefits and outcomes attributed to the system) and efficiency (the relationship between resources implemented and results obtained) (4,Peyre_TRVA_2019?).\nBackground: Effectiveness emerged as a core evaluation criterion through the adaptation of general program evaluation principles to public health surveillance. The concept gained prominence in surveillance evaluation frameworks during the late 20th and early 21st centuries, particularly through influential guidance documents from the Centers for Disease Control and Prevention (CDC) and the World Health Organization (WHO), which established effectiveness as a central measure for assessing surveillance system quality (4,WorldHealthOrganization_OtWF_2004?). These frameworks positioned effectiveness as one of three core criteria for assessing public policies and programs, alongside relevance and efficiency, reflecting broader trends in public sector performance evaluation (4). The operationalization of effectiveness in surveillance contexts evolved from general notions of \"achieving objectives\" to more specific technical performance measures. In surveillance evaluation, effectiveness became understood as the technical performance of the system—the capacity to reach its objective (such as disease control or early detection) through appropriate use of inputs to produce desired outputs (4,Drewe_EoAaPHS_2012?). This conceptualization linked effectiveness to measurable attributes such as sensitivity, timeliness, and specificity, allowing evaluators to quantify technical performance. The development of economic evaluation frameworks, particularly cost-effectiveness analysis (CEA), further formalized effectiveness as a measurable construct, using performance indicators as proxies for benefit when comparing surveillance investments (4,Grosbois_ARtUMoE_2015?). From a policy perspective, this evolution enabled effectiveness to measure the effects (outcomes and impacts) directly attributable to surveillance system implementation, establishing the principle that surveillance systems must be regularly evaluated to ensure they remain both effective and efficient in meeting evolving public health needs.\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#efficiency",
    "href": "dictionary_documentation.html#efficiency",
    "title": "dictionary documentation",
    "section": "Efficiency",
    "text": "Efficiency\nID: efficiency\nSee also: cost; benefit; value; resource allocation; acceptability (economic)\nSynonyms\n\nExact: economic efficiency, resource efficiency\nBroad: cost-effectiveness\n\nDefinition: The relationship between the resources consumed by a surveillance system and the results it produces (3).\nDescription: An efficient surveillance system accomplishes its objectives with minimum expenditure of time, human effort, and cost relative to outputs and benefits generated (Drewe_EaI_2012?). Surveillance incurs both direct costs (personnel, laboratory consumables, travel, equipment) and indirect costs (administrative overhead, opportunity costs), which must be evaluated against outputs such as disease detections, notifications, and actionable intelligence (CDC_UpdGuideEval_2001?). Economic efficiency represents the optimal balance between resources invested and information quality produced, considering performance attributes such as sensitivity, precision, and timeliness (5,6). Economic efficiency can be conceptualized at three hierarchical levels: optimization (maximizing net societal benefit), acceptability (ensuring benefits exceed costs), and cost-minimization (achieving technical targets at lowest cost) (3). Efficiency evaluation requires comparing system performance against alternatives or a counterfactual scenario to determine added value (6). Since surveillance is an intermediate product that informs intervention decisions, optimal evaluation considers the combined costs and benefits of both surveillance and intervention activities (5).\nBackground: Economic efficiency emerged as a formal evaluation attribute following recognition that public health resources are constrained and require optimal allocation (5,Drewe_EaI_2012?). Early guidelines identified cost as a consideration but did not mandate formal economic analysis (7). The 2001 CDC updated guidelines recommended judging costs relative to benefits, though comprehensive analyses remained uncommon (CDC_UpdGuideEval_2001?). Formalization as a distinct attribute occurred primarily through animal health frameworks between 2004-2015. The OASIS tool defined efficiency as the link between resources and results, establishing it as a value attribute (8). The SERVAL framework refined this by articulating three levels of economic efficiency and integrating economic evaluation as core to surveillance assessment (5). The RISKSUR project and EVA tool synthesized economic theory and practical methods, establishing that surveillance value derives from enabling intervention decisions and requiring evaluation of the surveillance-intervention-mitigation pathway (1,6). Contemporary approaches emphasize integrated evaluation considering monetary and non-monetary benefits, value of information, and the three-variable relationship between surveillance inputs, intervention actions, and loss avoidance (1,6).\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#engagement",
    "href": "dictionary_documentation.html#engagement",
    "title": "dictionary documentation",
    "section": "Engagement",
    "text": "Engagement\nID: engagement\nSee also: acceptability; collaboration\nSynonyms\n\nExact: participation\nBroad: –\n\nDefinition: The active involvement of stakeholders, including partners and communities, in the design, operation, interpretation, and use of the surveillance system.\nDescription: Engagement reflects the depth and quality of participation in surveillance activities. It encompasses contributions to data collection, system design, analysis interpretation, and application of surveillance outputs. Effective engagement is characterized by meaningful interaction, shared responsibility, and mutual benefit among all partners.\nBackground: The concept of engagement has evolved in surveillance practice, particularly in emphasizing community involvement. Engaged communities participate in various aspects of surveillance, from identifying priorities to collecting data and using results. Evidence of engagement includes consistent participation, valuable feedback, system adaptation based on community input, and local ownership of activities. Modern approaches recognize that engagement should include diverse communities, especially those most affected by health conditions under surveillance. This shift values equitable partnerships where communities are active collaborators rather than simply sources of data.\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#equity-data",
    "href": "dictionary_documentation.html#equity-data",
    "title": "dictionary documentation",
    "section": "Equity data",
    "text": "Equity data\nID: equityData\nSee also: representativeness; acceptability; data quality; usefulness; coverage\nSynonyms\n\nExact: equity-relevant data, health equity data, disparities data\nBroad: social determinants data, demographic data, stratification variables, population characteristic data, vulnerability indicators\n\nDefinition: The surveillance system’s data content necessary to capture population characteristics and contextual factors that enable analysis of differential health impacts across demographic and socioeconomic groups.\nDescription: Equity-relevant data ensures the system collects information necessary to analyze how disease burden, exposure, or surveillance outcomes vary across populations. For environmental surveillance systems, this requires data that can be spatially or demographically linked to affected communities, stratified by indicators of vulnerability such as socioeconomic status, age distribution, or geographic marginalization. For human health surveillance, this extends to demographic attributes (race/ethnicity, gender, age), and in specialized frameworks, may include measures of structural determinants such as racism or stigma. For animal health surveillance, relevant factors include producer characteristics, economic constraints, and cultural factors (e.g., livestock’s role in local economies) that influence disease reporting and system participation. The attribute assesses whether the system’s data infrastructure enables identification of which populations are disproportionately affected and supports development of targeted, equitable interventions. This directly influences representativeness (ensuring no groups are systematically excluded) and acceptability (ensuring culturally appropriate engagement).\nBackground: The concept of equity-relevant data in surveillance evaluation has three distinct but converging lineages. Traditional Public Health Surveillance (1990s-2000s): Foundational CDC guidelines from the 1990s required surveillance evaluation to consider disparities or inequities and assess data distribution by population demographics (race, ethnicity, age, socioeconomic status). This was traditionally addressed within the attribute of representativeness, which required collecting demographic characteristics and identifying systematically excluded population subgroups. One Health and Animal Health Frameworks (2013-present): Animal health and One Health evaluation frameworks identified that traditional approaches, focused on technical attributes like sensitivity, failed to capture the social and economic aspects influencing system performance. These frameworks required characterizing socioeconomic context, including local value systems, cultural importance of livestock, and economic constraints—factors that fundamentally shape surveillance effectiveness through their influence on acceptability and data quality. Health Equity and Anti-Racism Frameworks (2021-present): Public Health Critical Race Praxis (PHCRP) evaluation frameworks identified a critical gap: even systems collecting race/ethnicity data often fail to capture structural drivers of health inequities. PHCRP explicitly requires assessing whether systems capture measures of racism (such as residential segregation or discriminatory policies) and validated stigma indicators, moving beyond demographic categories to measure mechanisms producing inequitable outcomes. Convergence: These lineages converge in recognizing that adequate evaluation requires assessing whether systems collect data sufficient to identify who is disproportionately affected (demographics), why disparities exist (structural and contextual factors), and whether the system engages populations equitably (cultural appropriateness). Specific data elements vary by surveillance type, but the principle is consistent: equity-relevant data collection addresses not only demographic completeness but also the social, economic, and structural determinants shaping health outcomes and surveillance performance. This concept intersects with multiple traditional evaluation attributes including representativeness, acceptability, data quality, and usefulness.\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#false-alarm-rate",
    "href": "dictionary_documentation.html#false-alarm-rate",
    "title": "dictionary documentation",
    "section": "False-alarm rate",
    "text": "False-alarm rate\nID: falseAlarmRate\nSee also: specificity\nSynonyms\n\nExact: Inverse of specificity, False positive rate (FPR)\nBroad: Predictive value positive (PVP) - related inverse concept; low PVP indicates high false-alarm rate\n\nDefinition: The proportion of negative events (e.g. non-outbreak periods) incorrectly classified as events (outbreaks) (3,6).\nDescription: The false-alarm rate is the proportion of non-events incorrectly flagged as events and represents the inverse of specificity (3,5,6). While some false positives are expected in surveillance systems, a high false-alarm rate can lead to wasted resources through misdirected investigations of non-events (anonymous.2001?,Buehler_EOPHS_2004?,Groseclose_EOSH_2008?). Conversely, efforts to minimize false alarms by raising detection thresholds may compromise sensitivity, increasing the risk of missed outbreaks (Buehler_EOPHS_2004?,Groseclose_SSEDSHD_2013?). This trade-off is particularly critical in early detection and syndromic surveillance systems, where high data volumes and low event prevalence often generate elevated false-positive rates (Groseclose_SSEDSHD_2013?). Excessive false alerts can lead to alert fatigue among users, potentially causing genuine events to be missed, and may erode credibility of public health agencies (Lucero-Obusan_BMC_2022?,Groseclose_SSEDSHD_2013?). System designers should specify desired sensitivity and false-alarm rate thresholds, explicitly balancing the risk of missed outbreaks against the cost of investigating false alarms (Buehler_EOPHS_2004?). The false-alarm rate is closely related to predictive value positive (PVP), as a low PVP directly reflects a high false-alarm rate; both metrics quantify the quality of positive signals in a surveillance system (Buehler_EOPHS_2004?,anonymous.2001?).\nBackground: The term \"false-alarm rate\" was proposed and adopted in animal health surveillance evaluation frameworks as an alternative to \"specificity\" because it was considered more easily understood by non-epidemiologists, including surveillance planners and policy makers (3,6). The concept gained prominence as a key metric within the Evidence Quality or Effectiveness domain of surveillance evaluation frameworks, particularly for systems designed to detect early outbreaks or unexpected events (6,icahs-workshop-report?). In biosurveillance and outbreak detection systems, quantifying the false-alarm rate has been identified as essential for economic evaluations, with stakeholders demonstrating willingness-to-pay for improvements in this attribute (delRioVilas_PfEoOHSTEB_2022?,Amato_EPI_2023?).\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#feasibility",
    "href": "dictionary_documentation.html#feasibility",
    "title": "dictionary documentation",
    "section": "Feasibility",
    "text": "Feasibility\nID: feasibility\nSee also: sustainability; stability; efficiency; effectiveness\nSynonyms\n\nExact: N/A\nBroad: Practicality (ease of operation and administration), Resource adequacy, Implementability\n\nDefinition: The extent to which available means (resources, expertise, infrastructure) meet the surveillance system’s needs, ensuring that strategies and methods are suitable and applicable to the surveillance objectives (Yang_UOaHS_2022?,Drewe_EoAaPHS_2012?).\nDescription: Feasibility evaluates whether a surveillance system is realistic and sustainable given available resources, expertise, and objectives (Yang_UOaHS_2022?). It addresses the fundamental question of whether the means necessary to run the system are available and whether system strategies correspond to public health priorities (Drewe_EoAaPHS_2012?,Yang_UOaHS_2022?). Key components of feasibility assessment include staff capability and multidisciplinary expertise essential for system operation and maintenance, as well as whether personnel have the required knowledge, skills, and experience (Yang_UOaHS_2022?). Feasibility also considers the availability of necessary external resources, such as steering committees, technical committees, and government affiliations that support system implementation. The ability to track surveillance events over time using available data sources represents another critical feasibility consideration (Yang_UOaHS_2022?). Ultimately, feasibility assesses whether available resources are sufficient to support all system components—including data collection, analysis, and dissemination—effectively and efficiently. Limited resources pose significant constraints on achieving ideal surveillance criteria, making feasibility a primary consideration when setting evaluation objectives and determining system design (Yang_UOaHS_2022?).\nBackground:\n\nGovernance represents a relatively recent addition to surveillance evaluation frameworks, emerging in response to the complexity of multisectoral and One Health surveillance systems (9,10). Unlike classical epidemiological attributes (sensitivity, timeliness) that emerged from foundational frameworks with consistent definitions, governance developed without a unified evolutionary path, appearing variably across evaluation tools with context-dependent conceptualizations. Early surveillance evaluation recognized managerial and administrative weaknesses as obstacles but did not formalize governance as a distinct assessment domain (11).\n\n\nThe explicit formalization of governance as an evaluation attribute occurred primarily through One Health and integrated surveillance frameworks developed between 2010-2020. The ECoSur tool specifically targeted governance of collaboration in multisectoral systems, while NEOH, ATLASS, and PMP-AMR incorporated governance-related dimensions with varying emphases (9,10,12–14). These tools reflect three distinct but overlapping conceptualizations: structural governance (steering committees, formal agreements), functional governance (decision-making processes, resource allocation), and social governance (stakeholder representation, power dynamics, real versus official practices) (1,12,Figuie_AHSFCtRG_2022?).\n\n\nGovernance continues to evolve toward greater recognition of the gap between formal structures and actual practice, with emerging emphasis on adaptive capacity, stakeholder engagement authenticity, and transparent accountability mechanisms suited to complex health security challenges (1,Figuie_AHSFCtRG_2022?). Future development likely will focus on standardizing assessment approaches while maintaining flexibility for diverse surveillance contexts.\n\n\nGovernance overlaps substantially with organizational structure, collaboration, and system management concepts that appear in earlier frameworks (8,Figuie_AHSFCtRG_2022?). Its distinctiveness lies in three key elements: (1) explicit focus on power dynamics and decision-making authority rather than mere structural arrangements; (2) emphasis on the policy-institutional-operational continuum that links strategic intent to implementation; and (3) recognition of formal versus actual governance practices, distinguishing official rules from how systems truly function (12,Figuie_AHSFCtRG_2022?).\n\n\nGovernance serves as an enabling attribute that directly influences collaboration by providing multisectoral structures, supports sustainability through strategic resource planning, and affects acceptability through stakeholder engagement (12,13,15). Poor governance cascades into failures across multiple attributes—inadequate stakeholder engagement leads to poor acceptability and implementation challenges (15). Unlike traditional \"government\" (hierarchical authority) or \"management\" (technical operations), governance implies flexible power-sharing among diverse stakeholders (Figuie_AHSFCtRG_2022?).\n\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#flexibility",
    "href": "dictionary_documentation.html#flexibility",
    "title": "dictionary documentation",
    "section": "Flexibility",
    "text": "Flexibility\nID: flexibility\nSee also: portability; simplicity; sustainability; acceptability\nSynonyms\n\nExact: adaptability\nBroad: scalability\n\nDefinition: The ability of the system to adapt to changing information needs or operating conditions, including scaling up or down as necessary, with little additional time, personnel, or funds (Drewe_EoAaPHS_2012?,Groseclose_EPHS_2010?,Azofeifa_EBHS_2018?,Hoinville_TRVA_2013?).\nDescription: Flexibility is the system’s ability to adapt to new information needs, emerging threats, or changing resources with minimal disruption. Flexible systems must accommodate diverse changes including new health-related events or hazards, modifications to case definitions or technologies, and variations in funding or reporting sources (Groseclose_EPHS_2010?,Amato_E_2023?). Examples of such adaptations include responding to changes in data standards (e.g., ICD-9 to ICD-10), incorporating new data sources and technologies (e.g., online data sharing or machine learning techniques), or adjusting case definitions to reflect evolving understanding of health conditions (Groseclose_EPHS_2010?). Standardized formats and modular designs support flexibility by providing robust, adaptable infrastructure. Systems utilizing standard vocabularies or standardized formats (such as PHIN standards) provide flexible code structures that can accommodate changing information needs. Centralized data processing can enhance flexibility by reducing the need for widespread system and operator behavior changes when adaptations are required. Flexibility is generally higher in simpler systems, as fewer components require modification when adapting (Thacker_MEES_1988?). Flexibility is closely related to sustainability, resilience, and system responsiveness. Flexible systems are vital for long-term maintenance, especially when facing external factors such as funding variations or personnel cuts, with infrastructure that can evolve as the system grows. Flexibility also supports acceptability and usefulness, as systems that can be tailored to meet the evolving needs and interests of staff and stakeholders foster greater participation and sustained engagement.\nBackground: Flexibility emerged as one of the seven original surveillance system attributes identified by Thacker et al. in 1988, establishing it as a foundational evaluation criterion (Thacker_MEES_1988?,Groseclose_EPHS_2010?). The attribute has been consistently included in surveillance evaluation frameworks across public health, animal health, and One Health systems, reflecting its status as a core performance measure. It was retained as one of nine core metrics in the modernized US CDC Evaluation Guidelines and continues to appear in contemporary evaluation tools (Groseclose_EPHS_2010?,Peyre_TRVA_2019?). The definition of flexibility has remained highly consistent since its introduction, emphasizing low-cost adaptation to changing conditions with minimal additional resources (Drewe_EoAaPHS_2012?,Groseclose_EPHS_2010?,Hoinville_TRVA_2013?). The term is often used interchangeably with \"adaptability\" or referenced as \"adaptability to changes,\" and some evaluation tools formally list the attribute as \"Flexibility, adaptability\" (Peyre_TRVA_2019?). Recent applications have expanded the concept to explicitly include the system’s ability to scale up, scale down, or expand as necessary, particularly relevant for emerging surveillance contexts such as wastewater surveillance (Amato_E_2023?). Flexibility’s relationship to other evaluation attributes reveals its interconnected role in system performance. The attribute demonstrates a bidirectional relationship with simplicity—simpler systems exhibit greater flexibility because fewer components require modification during adaptation (Thacker_MEES_1988?). Flexibility also supports sustainability by enabling systems to persist despite changing external factors such as funding variations or staffing changes, while simultaneously contributing to acceptability and usefulness by allowing systems to be tailored to stakeholder needs. The relative importance of flexibility may vary across system development stages, with some evidence suggesting it holds particular relevance during initial development and pilot testing, while attributes like reliability and stability assume greater significance during operational maturity (Auer_R_2011?).\nMeasurement: Flexibility is probably best evaluated retrospectively by observing how a system has responded to a new demand. Unless efforts have been made to adapt the public health surveillance system to another disease (or other health-related event), a revised case definition, additional data sources, new information technology, or changes in funding, assessing the flexibility of that system might be difficult. In the absence of practical experience, the design and workings of a system can be examined. Simpler systems might be more flexible (i.e., fewer components will need to be modified when adapting the system for a change in information needs or operating conditions)."
  },
  {
    "objectID": "dictionary_documentation.html#governance",
    "href": "dictionary_documentation.html#governance",
    "title": "dictionary documentation",
    "section": "Governance",
    "text": "Governance\nID: governance\nSee also: collaboration; acceptability; sustainability; organizational structure; engagement; resource allocation; accountability; transparency; multisectoral coordination; intersectoral collaboration\nSynonyms\n\nExact: system governance, surveillance governance\nBroad: N/A\n\nDefinition: The coordination, leadership, and operational effectiveness of surveillance systems, encompassing the legislative framework, steering mechanisms, and organizational structures that ensure clear roles, shared responsibilities, and inclusive decision-making to achieve surveillance objectives (9,Figuie_AHSFCtRG_2022?).\nDescription:\n\nGovernance refers to the coordination, leadership, and operational effectiveness of a surveillance system, encompassing how it is organized, steered, and managed (9,16). Strong governance ensures clear roles and responsibilities, shared accountability, inclusive decision-making, and formalized collaboration across sectors and disciplines (12,Ruegg_GfEiS_2022?).\n\n\nEffective governance operates at three levels: the policy level defines collaborative strategy and goals; the institutional level establishes governance modalities (steering mechanisms, coordination, technical support); and the operational level implements routine collaborative activities (12). These are documented through regulations, charters, or formal agreements with clearly defined roles in structures such as steering committees (8,12). Strong governance requires defining strategic goals, organizing resource allocation, ensuring system adaptivity through evaluation, and managing compliance with legal, ethical, and confidentiality requirements (17,Figuie_AHSFCtRG_2022?).\n\n\nGovernance mechanisms define the structure and quality of collaboration in multisectoral systems, making them particularly critical for One Health approaches where strategies must be organized at policy and institutional levels (12,13). Unlike hierarchical \"government\" authority, governance promotes flexible power-sharing among stakeholders (Figuie_AHSFCtRG_2022?). Understanding governance in practice—not just official rules—enables effective system design (1,Figuie_AHSFCtRG_2022?). Poor governance, such as inadequate stakeholder engagement, cascades into poor acceptability and implementation failures (15), while good governance enables collaboration, supports sustainability through resource planning, and enhances effectiveness through accountability (12,13).\n\nBackground:\n\nGovernance represents a relatively recent addition to surveillance evaluation frameworks, emerging in response to the complexity of multisectoral and One Health surveillance systems (9,10). Unlike classical epidemiological attributes (sensitivity, timeliness) that emerged from foundational frameworks with consistent definitions, governance developed without a unified evolutionary path, appearing variably across evaluation tools with context-dependent conceptualizations. Early surveillance evaluation recognized managerial and administrative weaknesses as obstacles but did not formalize governance as a distinct assessment domain (11).\n\n\nThe explicit formalization of governance as an evaluation attribute occurred primarily through One Health and integrated surveillance frameworks developed between 2010-2020. The ECoSur tool specifically targeted governance of collaboration in multisectoral systems, while NEOH, ATLASS, and PMP-AMR incorporated governance-related dimensions with varying emphases (9,10,12–14). These tools reflect three distinct but overlapping conceptualizations: structural governance (steering committees, formal agreements), functional governance (decision-making processes, resource allocation), and social governance (stakeholder representation, power dynamics, real versus official practices) (1,12,Figuie_AHSFCtRG_2022?).\n\n\nGovernance continues to evolve toward greater recognition of the gap between formal structures and actual practice, with emerging emphasis on adaptive capacity, stakeholder engagement authenticity, and transparent accountability mechanisms suited to complex health security challenges (1,Figuie_AHSFCtRG_2022?). Future development likely will focus on standardizing assessment approaches while maintaining flexibility for diverse surveillance contexts.\n\n\nGovernance overlaps substantially with organizational structure, collaboration, and system management concepts that appear in earlier frameworks (8,Figuie_AHSFCtRG_2022?). Its distinctiveness lies in three key elements: (1) explicit focus on power dynamics and decision-making authority rather than mere structural arrangements; (2) emphasis on the policy-institutional-operational continuum that links strategic intent to implementation; and (3) recognition of formal versus actual governance practices, distinguishing official rules from how systems truly function (12,Figuie_AHSFCtRG_2022?).\n\nMeasurement: Governance assessment remains less standardized than classical surveillance attributes, varying substantially by context, institutional complexity, and evaluation purpose. Measurement approaches include: (1) structural assessment through document review of policies, charters, and formal agreements (8,12); (2) functional evaluation using semi-quantitative scoring of governance criteria across multiple dimensions, as implemented in ECoSur’s 22 organizational attributes and 3 governance indexes (12,13); and (3) qualitative assessment through stakeholder interviews and participatory methods to understand real versus official governance (1,Figuie_AHSFCtRG_2022?). The challenge in governance measurement lies in capturing both formal structures and actual practices, requiring mixed-methods approaches that combine document analysis, stakeholder perspectives, and observation of decision-making processes."
  },
  {
    "objectID": "dictionary_documentation.html#granularity",
    "href": "dictionary_documentation.html#granularity",
    "title": "dictionary documentation",
    "section": "Granularity",
    "text": "Granularity\nID: granularity\nSee also: completeness\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The level of detail provided in input sources and output data of the surveillance system.\nDescription: –\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#historical-data",
    "href": "dictionary_documentation.html#historical-data",
    "title": "dictionary documentation",
    "section": "Historical data",
    "text": "Historical data\nID: historicalData\nSee also: accessibility (data); data quality; data management and storage\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: Refers to the quality and accessibility of archived data.\nDescription: Effective governance operates at three levels: the policy level defines collaborative strategy and goals; the institutional level establishes governance modalities (steering mechanisms, coordination, technical support); and the operational level implements routine collaborative activities (12). These are documented through regulations, charters, or formal agreements with clearly defined roles in structures such as steering committees (8,12). Strong governance requires defining strategic goals, organizing resource allocation, ensuring system adaptivity through evaluation, and managing compliance with legal, ethical, and confidentiality requirements (17,Figuie_AHSFCtRG_2022?).\nBackground: TBD\nMeasurement: Questions to consider include: * How many years of data are stored? * How complete and reliable are the data? * Are the data stored in a way that allows the required interrogation and analysis? * Is there a summary overview of the data and collection methods explaining key idiosyncrasies of the data and changes to the data or collection methods over time? * What use is currently made of historical surveillance data?"
  },
  {
    "objectID": "dictionary_documentation.html#impact",
    "href": "dictionary_documentation.html#impact",
    "title": "dictionary documentation",
    "section": "Impact",
    "text": "Impact\nID: impact\nSee also: Effectiveness; Benefit\nSynonyms\n\nExact: usefulness, utility\nBroad: benefit, value\n\nDefinition: The measure of changes that have been made based on surveillance results in relation to the system’s aims, including the actions taken and outcomes achieved as a result of the information provided (4,icahs-workshop-report?,Drewe_SERVAL_2015?).\nDescription: –\nBackground: Impact and usefulness represent closely related concepts that measure the ultimate value of surveillance systems, with their relationship reflecting different evaluation traditions and contexts rather than fundamental conceptual differences. Historically, evaluating surveillance systems included assessing usefulness by asking practitioners about system value, but more rigorous approaches involved assessing the impact of surveillance data on policies and interventions (Thacker_MEES_1988?,Thacker_PPHS_1988?). The term \"impact\" gained particular prominence in animal health surveillance, economic evaluation, and One Health contexts, where it became the preferred terminology in frameworks like SERVAL and EVA Survtool that assess ultimate value and benefits achieved (4,Hoinville_TRVA_2013?,Drewe_SERVAL_2015?). Impact is often defined specifically as the changes made based on surveillance results or the consequences beyond immediate effectiveness, relating to societal benefits of loss avoidance (Drewe_SERVAL_2015?,Meynard_PFAFM_2008?). This framing connects impact directly to economic evaluation methodologies, linking performance (effectiveness) to monetary benefit (cost-benefit analysis) or non-monetary benefit (cost-effectiveness analysis) (Peyre_TRVA_2019?,Drewe_SERVAL_2015?). Impact evaluation methods seek to answer cause-and-effect questions by comparing outcomes to counterfactuals—what would have happened without the surveillance intervention. In integrated surveillance frameworks like the Integrated Surveillance System Evaluation Project (ISSEP), impact represents a specific evaluation level (Level 5: Contribution to Desirable Outcomes), where ultimate outcomes include improved animal, human, and environmental health, along with associated socioeconomic benefits (Bennani_EISA_2021?,Sandberg_AoEFTfIS_2021?). Modern surveillance evaluation increasingly emphasizes impact assessment, driven by both donor accountability requirements and social accountability demands (4,Peyre_AITAEoPHS_2022?). Workshop participants assessing evaluation attribute importance have generally considered impact alongside costs as highly relevant attributes for system performance, sometimes viewing it as conceptually distinct from organizational benefits (Hoinville_TRVA_2013?). While impact and usefulness are explicitly recognized as synonyms in evaluation literature, impact tends to be emphasized in contexts requiring quantifiable evidence of return on investment and in integrated or One Health systems where demonstrating the value of cross-sectoral collaboration is essential. The attribute’s dependence on virtually all other surveillance qualities (sensitivity, timeliness, representativeness, acceptability, stability) reflects its role as a summary measure of whether the entire surveillance enterprise achieves its intended purpose of improving health outcomes.\nMeasurement: Evaluating the impact involves examining real-world examples where the system’s data has influenced public health actions, such as policy changes or control measures. This assessment often requires retrospective analysis and gathering input from key stakeholders to understand the system’s effectiveness. Key questions include how well the system’s objectives align with policy and industry needs, how outputs are utilized, and whetherstakeholder information needs have been met. Additionally, the evaluation should examine the system’s influence on the development of disease control policies, the prioritization of health threats, and the early detection or mitigation of outbreaks. It is also important to detail actions taken based on the system’s findings, such as changes in protocols or behaviors. The evaluation should assess the extent to which the system’s objectives have been achieved, providing a clear understanding of its impact on public health and disease management. Retrospective assessments and stakeholder feedback help evaluate whether the system’s outputs have guided effective actions. Regular review of system performance and stakeholder input helps ensure continued relevance and effectiveness in addressing public health threats. Retrospective assessment through stakeholder input helps capture real-world benefits, such as improved disease detection, control, or prioritization."
  },
  {
    "objectID": "dictionary_documentation.html#interoperability",
    "href": "dictionary_documentation.html#interoperability",
    "title": "dictionary documentation",
    "section": "Interoperability",
    "text": "Interoperability\nID: interoperability\nSee also: accessibility (data); data management and storage\nSynonyms\n\nExact: compatibility, integration\nBroad: N/A\n\nDefinition: Compatibility with and ability to integrate data from other sources and surveillance components.\nDescription: Interoperability helps ensure timely, efficient data exchange across sectors, disciplines, and administrative entities. Interoperability supports joint analysis, enhances flexibility, and improves sustainability. Effective interoprability requires standardized data formats, unique identifiers (e.g., sample or location IDs), and reliable data-sharing mechanisms. Challenges include fragmented or duplicative systems, lack of digital integration, or a lack of formal communication pathways. The importance of interoperability may depend on how essential inter-system data exchange is to the overall usefulness and effectiveness of the surveillance system.\nBackground: TBD\nMeasurement: Description of the elements that are integrated in the surveillance system (Focus groups or individual interviews). Evaluation of the relevance and rationale of the integration with regards to surveillance objectives (Focus groups or individual interviews). Evaluation of the governance and multi-sectoral collaboration in the system (Focus groups or individual interviews, complementary tools). Overall evaluation of the level of integration (Semi-quantitative measurement scale, complementary tools)."
  },
  {
    "objectID": "dictionary_documentation.html#laboratory-and-sample-management",
    "href": "dictionary_documentation.html#laboratory-and-sample-management",
    "title": "dictionary documentation",
    "section": "Laboratory and sample management",
    "text": "Laboratory and sample management\nID: laboratoryAndSampleManagement\nSee also: technical competence and training\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: Whether testing is carried out using appropriate methods with quality assurance scheme and timely and accurate delivery of results.\nDescription: Laboratory and sample management evaluates the integration, reliability, and quality of diagnostic laboratory functions within the surveillance system. Key considerations include the implementation of systematic quality management practices, participation in proficiency testing, and adherence to international standards (e.g., ISO 9001, ISO 17025). Effective sample handling, timely analysis, and standardized protocols and harmonization of techniques across laboratories support data comparability. Laboratory and sample management is also closely related to technical competence and training.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#likelihood-ratio-of-a-positive-test",
    "href": "dictionary_documentation.html#likelihood-ratio-of-a-positive-test",
    "title": "dictionary documentation",
    "section": "Likelihood ratio of a positive test",
    "text": "Likelihood ratio of a positive test\nID: likelihoodRatioOfAPositiveTest\nSee also: TBD\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: Ratio of the probability of a surveillance system detecting an infected individual to the probability of the system incorrectly identifying them as infected when they are in fact not.\nDescription: Likelihood ratios do not vary with disease prevalence and so are stable expressions of system performance.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#multiple-utility",
    "href": "dictionary_documentation.html#multiple-utility",
    "title": "dictionary documentation",
    "section": "Multiple utility",
    "text": "Multiple utility\nID: multipleUtility\nSee also: flexibility; portability\nSynonyms\n\nExact: multiple hazard\nBroad: N/A\n\nDefinition: The ability of a surveillance system to capture information on several hazards; measure of how generic the system is.\nDescription: Multiple utility describes a surveillance system’s capacity to collect information on several hazards, reflecting how generic and adaptable it is. Systems with high multiple utility can support various surveillance objectives, improving cost-effectiveness and overall value. This term is positively linked to acceptability, sustainability, flexibility, and cos. Both realised and potential multiple utility should be assessed to identify opportunities for improvement. Systems with broad, representative designs generally offer greater multiple utility than those narrowly focused on specific risks.\nBackground: TBD\nMeasurement: An assessment of multiple utility should consider: - What additional information is or could be gathered during sample collection (eg on animal health or husbandry and demographics)? - What other types of samples are or could be collected at the time of sampling? - What other diseases are or could be tested for with the samples collected? - How long are samples stored following testing and could they be used for other purposes (including other research purposes)? For a surveillance system to offer value to other diseases or information needs, the objectives and processes of the system should be aligned to other systems. So it may be expected that more simple systems are likely to have more potential for multiple utility. For example, a simple random survey of holdings, repeated annually and with good coverage and representativeness could be useful for various diseases; whereas a risk-based design aimed at a specific threat may be of limited value for other diseases with differing epidemiology."
  },
  {
    "objectID": "dictionary_documentation.html#mutual-benefit",
    "href": "dictionary_documentation.html#mutual-benefit",
    "title": "dictionary documentation",
    "section": "Mutual benefit",
    "text": "Mutual benefit\nID: mutualBenefit\nSee also: collaboration\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The capability of the surveillance system to create opportunities where collaborations can benefit both the system and its partners/collaborators.\nDescription: Mutual Benefit refers to the extent to which all collaborators gain value from participating in the surveillance system. This includes the availability of shared data infrastructure, clarity in roles and responsibilities, and the fostering of trusted relationships. Mutual understanding of privacy, confidentiality, and security processes supports sustained cooperation. Systems that enable data exchange and define partner benefits are more likely to promote long-term collaboration and stakeholder engagement.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#negative-predictive-value-npv",
    "href": "dictionary_documentation.html#negative-predictive-value-npv",
    "title": "dictionary documentation",
    "section": "Negative predictive value (NPV)",
    "text": "Negative predictive value (NPV)\nID: negativePredictiveValue\nSee also: positive predictive value\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The probability that no health event is present given that no health event is detected.\nDescription: Negative predictive value (NPV) reflects the likelihood that a negative surveillance result truly indicates absence of condition, hazard, or event under surveillance. A high NPV suggests few false negatives and supports confidence in the system’s ability to rule out threats when no signal is detected. NPV is influenced by the system’s sensitivity, specificity, and the prevalence of the condition in the population. IIt applies acros various public health surveillance modalities, including lagroatory, syndomic, environmental, and even-based approaches, and should be considered alongside sensitivity and positive predictive value.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#performance-monitoring-and-evaluation",
    "href": "dictionary_documentation.html#performance-monitoring-and-evaluation",
    "title": "dictionary documentation",
    "section": "Performance monitoring and evaluation",
    "text": "Performance monitoring and evaluation\nID: performanceMonitoringAndEvaluation\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The routine use of performance indicators to monitor surveillance system performance, and the periodic use of internal or external evaluations to assess system outputs in relation to stated objectives.\nDescription: Performance monitoring and evaluation assesses the extent to which the surveillance system uses performance indicators and scheduled evaluations to track progress, measure effectiveness, and ensure accountability. This includes regular internal monitoring and external assessments of system components, including data quality, timeliness, sensitivity, and stakeholder collaboration. Performance monitoring and evaluation supports system transparency, enables corrective actions, and contributes to long-term improvement. It is closely linked to effectiveness, efficiency, impact, and sustainability.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#portability",
    "href": "dictionary_documentation.html#portability",
    "title": "dictionary documentation",
    "section": "Portability",
    "text": "Portability\nID: portability\nSee also: flexibility\nSynonyms\n\nExact: generalizability\nBroad: N/A\n\nDefinition: The ease with which a surveillance system can be replicated or implemented in a different context or setting.\nDescription: Portability describes how easily a surveillance system can be adopted or adapted in other settings or jurisdictions. High portability is supported by standardized formats, clear documentation, modular design, and minimal dependence on local infrastructure or specialized personnel. Systems that are portable enable broader uptake, facilitate scalability, and enhance knowledge sharing. Portability is positively associated with attributes such as simplicity, flexibility, and sustainability.\nBackground: TBD\nMeasurement: Examples should be provided of the deployment of similar systems in other settings, and the experience of those efforts should be described. In the absence of examples, features of the system that might support or detract from portability should be described."
  },
  {
    "objectID": "dictionary_documentation.html#positive-predictive-value-ppv",
    "href": "dictionary_documentation.html#positive-predictive-value-ppv",
    "title": "dictionary documentation",
    "section": "Positive predictive value (PPV)",
    "text": "Positive predictive value (PPV)\nID: positivePredicitiveValue\nSee also: negative predictive value\nSynonyms\n\nExact: N/A\nBroad: sensitivity\n\nDefinition: The proportion of reported cases that actually have the health-related event under surveillance.\nDescription: Positive Predictive Value (PPV) is the proportion of cases identified by a surveillance system that are true cases. A high PPV indicates fewer false positives, which helps minimize unnecessary investigtations and conserves resources. PPV is influenced by the specificity and sensitivity of case definitions, as well as the prevalence of the condition in the population. As sensitivity and PPV approach 100%, a system is more likely to be representative of the population with the event under surveillance. However, as sensitivity increases, PPV may decrease; therefore, a balance should be maintained between prioritizing early detection and accuracy. PPV may also influenced by data quality, communication, and reporting practices.\nBackground: TBD\nMeasurement: To assess PVP, external data sources, such as medical records, health claims, or registries, are often used as benchmarks to verify the true cases identified by the surveillance system. The assessment of sensitivity and of PVP provide different perspectives regarding how well the system is operating. Depending on the objectives of the public health surveillance system, assessing PVP whenever sensitivity has been assessed might be necessary (47-50,53 ). In this report, PVP is represented by A/(A+B) (Table 3). Evaluating Positive Predictive Value (PVP) involves assessing the accuracy of case detection and outbreak identification based on reported data. This can be done on two levels: individual case detection and epidemic detection. For case detection, the PVP is determined by confirming reported cases through investigation and comparing the number of true cases to the total number of reported cases. This requires keeping records of all interventions triggered by the surveillance system, such as the number of case investigations conducted and the proportion of cases confirmed. For epidemic detection, the PVP is assessed by evaluating the proportion of detected outbreaks that are true epidemics. This can involve reviewing personnel activity reports, travel records, and telephone logbooks to determine if resources are being misdirected due to false-positive case reports. Additionally, external data sources, such as medical records or registries, may be used to validate reported cases and improve the accuracy of PVP calculations. To obtain a comprehensive understanding, it may be necessary to assess PVP for different data sources, system data fields, and specific health-related events. Regular analysis of these factors ensures that the system effectively uses resources and minimizes the impact of false-positive results."
  },
  {
    "objectID": "dictionary_documentation.html#precision",
    "href": "dictionary_documentation.html#precision",
    "title": "dictionary documentation",
    "section": "Precision",
    "text": "Precision\nID: precision\nSee also: accuracy; bias\nSynonyms\n\nExact: N/A\nBroad: repeatability, consistency\n\nDefinition: The degree to which a numerical estimate is narrowly defined, typically indicated by the tightness of its confidence interval.\nDescription: Precision reflects the level of statistical uncertainty around an estimate, with narrower confidence intervals indicating higher precision. It is influenced by sample size, confidence level, variability in the data, and overall data quality. Precision is important for interpreting the reliability of surveillance indicators and comparing system performance over time or across regions.\nBackground: TBD\nMeasurement: Evaluation of statistical stability (precision) involves calculation of relative standard error of the primary estimate"
  },
  {
    "objectID": "dictionary_documentation.html#preparedness",
    "href": "dictionary_documentation.html#preparedness",
    "title": "dictionary documentation",
    "section": "Preparedness",
    "text": "Preparedness\nID: preparedness\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: Preparedness corresponds to the set of actions that are taken as precautionary measures in the face of potential hazard-related issues.\nDescription: Preparedness refers to the extent to which a surveillance system is equipped to respond effectively to public health emergencies. This includes having written procedures, emergency trained personnel, and readily available resources to support timely action. A well-prepared system can detect, report, and respond to emerging threats quickly and efficiently. The preparedness term is closely linked to timeliness, stability, and sustainability, as it depends on ongoing capacity, planning, and coordination.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#redundancy",
    "href": "dictionary_documentation.html#redundancy",
    "title": "dictionary documentation",
    "section": "Redundancy",
    "text": "Redundancy\nID: redundancy\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The ability of the surveillance system to operate in the face of component loss or degradation.\nDescription: Redundancy refers to the presence of backup systems or components—such as staff, IT infrastructure, data sources, or interagency agreements—that ensure continued surveillance operations if one part fails. It supports system stability and resilience, allowing for uninterrupted performance during disruptions. Without redundancy, the failure of a single component can compromise data collection, analysis, or reporting. Redundancy is closely related to stability, robustness, and sustainability, as it enhances the system’s ability to maintain functionality and deliver reliable outputs under various conditions.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#relevance",
    "href": "dictionary_documentation.html#relevance",
    "title": "dictionary documentation",
    "section": "Relevance",
    "text": "Relevance\nID: relevance\nSee also: effectivness; usefulness\nSynonyms\n\nExact: pertienence\nBroad: N/A\n\nDefinition: The degree to which functions and activities in the surveillance system are relevant to its objectives and priorities.\nDescription: Relevance assesses how well a surveillance system’s functions, activities, and data align with its objectives, priorities, and user needs. It examines whether the system addresses key data gaps, supports decision-making, and reflects epidemiological, socio-political, and economic contexts. Relevance also involves ensuring collaborative activities (e.g., sampling, lab testing, data sharing) match the system’s goals and stakeholder expectations. High relevance enhances system usefulness, acceptability, and sustainability by focusing resources on priority surveillance needs.\nBackground: TBD\nMeasurement: Evaluation considers if unnecessary data collection is minimized and if performance indicators are linked to objectives."
  },
  {
    "objectID": "dictionary_documentation.html#repeatability",
    "href": "dictionary_documentation.html#repeatability",
    "title": "dictionary documentation",
    "section": "Repeatability",
    "text": "Repeatability\nID: repeatability\nSee also: N/A\nSynonyms\n\nExact: –\nBroad: consistency, precision\n\nDefinition: How consistently the study results can be reproduced over time.\nDescription: Repeatability is a key concept in validating diagnostic tests and is closely tied to precision. In the context of a surveillance system, it also relates to the terms/attributes of historical data, stability, and sustainability. A surveillance system that excels in repeatability generates data that can be consistently compared across years, with clear documentation of any changes in data collection methods or variables over time. This ensures that variations in the data are well-understood and attributable to actual trends rather than inconsistencies in data collection, facilitating accurate longitudinal analysis and comparison.\nBackground: TBD\nMeasurement: One might consider changes to legislation; changes to diagnostic methods, including improvements of adoption of new technology; changes to surveillance design; or influences on disease reporting behaviour in passive surveillance activities. - How have these impacted on the comparability of surveillance data over the time period of interest? - Have these influences been identified and examined and can they be accommodated in interpretation of the surveillance data?"
  },
  {
    "objectID": "dictionary_documentation.html#representativeness",
    "href": "dictionary_documentation.html#representativeness",
    "title": "dictionary documentation",
    "section": "Representativeness",
    "text": "Representativeness\nID: representativeness\nSee also: coverage\nSynonyms\n\nExact: N/A\nBroad: coverage?\n\nDefinition: The extent to which data adequately represent the population under surveillance and relevant sub-populations by time, place, population demographics and socio-demographics.\nDescription: Representativeness refers to how well surveillance data reflect the characteristics of the population of interest, allowing findings to be generalized without systematic bias. It considers factors like geography, demographics, and clinical presentation. While sensitivity focuses on detecting cases, representativeness ensures that the distribution of cases mirrors reality. High coverage across regions and groups supports inclusivity, while limited or uneven data can lead to selection or ascertainment bias. The representativeness of a surveillance system is related to both coverage and bias. Representativeness is also closely linked to terms such as sensitivity, precision, and data quality, and is essential for drawing accurate conclusions and informing effective public health planning.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#responsiveness-processes",
    "href": "dictionary_documentation.html#responsiveness-processes",
    "title": "dictionary documentation",
    "section": "Responsiveness (processes)",
    "text": "Responsiveness (processes)\nID: responsiveness\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: –\n\nDefinition: Responsiveness refers to the system’s ability to promptly address user requests and the willingness of support staff to assist with inquiries and provide necessary services.\nDescription: Responsiveness measures how quickly and effectively a surveillance system addresses user requests and support needs. It reflects the willingness and ability of staff to assist with inquiries, troubleshoot issues, and provide necessary services. High responsiveness supports timely data reporting, improves user satisfaction, and enhances system acceptability. Responsiveness is closely linked to timeliness and acceptability, ensuring the system remains user-friendly and operationally efficient.\nBackground: TBD\nMeasurement: Evaluation includes user feedback on response times and support quality, as well as tracking the availability and effectiveness of help resources."
  },
  {
    "objectID": "dictionary_documentation.html#robustness",
    "href": "dictionary_documentation.html#robustness",
    "title": "dictionary documentation",
    "section": "Robustness",
    "text": "Robustness\nID: robustness\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The ability of the surveillance system to produce acceptable outcomes over a range of assumptions about uncertainty by maximising the reliability of an adequate outcome.\nDescription: Robustness refers to a surveillance system’s ability to consistently generate reliable results over time, even under varying conditions and uncertainties. It ensures stable, accurate outcomes regardless of environmental or data changes. Robust systems are closely linked to precision, as both require dependable results. Additionally, robustness ties into consistency, flexibility, and portability, allowing the system to adapt to diverse contexts. A robust surveillance system is crucial for providing reliable data that supports long-term public health decisions and interventions, making it vital for effective and sustainable public health responses.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#security",
    "href": "dictionary_documentation.html#security",
    "title": "dictionary documentation",
    "section": "Security",
    "text": "Security\nID: security\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The privacy and data confidentiality measures required to prevent surveillance data compromise, based on relevant standards and guidelines.\nDescription: Security in a surveillance system involves safeguarding the privacy and confidentiality of data to prevent unauthorized access or breaches. Security measures should be reviewed regularly and adapted to emerging threats. Strong security practices ensures data integrity and builds trust between actors/stakeholders of the surveillance system.\nBackground: TBD\nMeasurement: Surveillance system security policies and practices should be reviewed to ensure that security levels and procedures for surveillance system data or system access are defined and enforced; data use and release policy and protocol is available; and access to the surveillance system software application is controlled."
  },
  {
    "objectID": "dictionary_documentation.html#sensitivity",
    "href": "dictionary_documentation.html#sensitivity",
    "title": "dictionary documentation",
    "section": "Sensitivity",
    "text": "Sensitivity\nID: sensitivity\nSee also: specificity\nSynonyms\n\nExact: recall (Morbey 2021), completeness (Groseclose 2010)\nBroad: positive predictive value\n\nDefinition: The system’s ability to accurately identify and capture all true cases or events of interest within the target population\nDescription: Sensitivity can be assessed at three levels: Case detection sensitivity – the proportion of actual cases (individuals or herds) with the condition that are correctly identified by the system; Outbreak detection sensitivity – the likelihood that the system will detect a significant increase in disease incidence, provided a clear definition of an outbreak exists; and Presence detection sensitivity – the probability that the system will detect the disease if it is present at or above a specified prevalence level in the population.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#simplicity",
    "href": "dictionary_documentation.html#simplicity",
    "title": "dictionary documentation",
    "section": "Simplicity",
    "text": "Simplicity\nID: simplicity\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: Refers to the surveillance system structure, ease of operation, and flow of data through the system.\nDescription: Simplicity should be inherent in the system as a whole, as well as each component (case definition, reporting procedures, etc.), to make it easy to understand and implement. In general, a surveillance system should be as simple as possible while still meeting its objectives. A simple system typically enhances flexibility, reduces resource requirements, and contributes to better timeliness. Simplicity also positively influences acceptability and engagement, as actors/stakeholders are more likely to adopt and participate in a system that is easy to use and understand. Simplicity also supports sustainability by easing training, maintenance, and management, ultimately leading to more accurate and comprehensive data.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#specificity",
    "href": "dictionary_documentation.html#specificity",
    "title": "dictionary documentation",
    "section": "Specificity",
    "text": "Specificity\nID: specificity\nSee also: false-alarm rate; sensitivity\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The proportion of cases correctly identified by the system as not having the health-related event under surveillance.\nDescription: Specificity refers to a surveillance system’s ability to correctly identify non-cases, minimizing false positives. It complements sensitivity by ensuring that resources are not wasted on investigating non-events. High specificity is especially important in outbreak detection and in systems monitoring rare or high-impact conditions. It reduces false alarms, conserving time and funding. Specificity is influenced by factors such as case definitions and population denominators. In some contexts, a trade-off with sensitivity may be acceptable to avoid missing critical threats.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#stability",
    "href": "dictionary_documentation.html#stability",
    "title": "dictionary documentation",
    "section": "Stability",
    "text": "Stability\nID: stability\nSee also: sustainability\nSynonyms\n\nExact: ability to contrinue tracking an event (sub-attribute), reliability, availability\nBroad: N/A\n\nDefinition: Stability refers to the reliability (ie, the ability to collect,manage, and provide data without failure) and availability (the ability to operate when needed) of the public health surveillancesystem.\nDescription: Stability refers to the system’s ability to consistently operate and deliver results over time, even under stress or changing conditions. It includes reliability, availability, and resilience to disruptions. Stability depends on factors such as workforce capacity, secure funding, legal frameworks, and continuity plans. It is positively correlated with sustainability and acceptability.\nBackground: TBD\nMeasurement: Measures for determining stability can include the number or duration of unscheduled outages of the information system(s) supporting the surveillance system; the comparison of the desired and actual amount of time or resources required for the system to collect, manage, analyze, interpret, and release data from the system; or the presence or absence of continuity of operations procedures intended to maintain system performance. This attribute can be measured retrospectively by 1. Looking at the incidence of minor and major faults over a defined period of time or 2. Giving a measure of the proportion of time that the system is fully functional Assessment of this attribute will benefit from consultation with those involved in the generation, management and analysis of surveillance data. If performance indicators have been implemented in the surveillance process, historical data from these will give a good insight into the ongoing functioning of the system."
  },
  {
    "objectID": "dictionary_documentation.html#sustainability",
    "href": "dictionary_documentation.html#sustainability",
    "title": "dictionary documentation",
    "section": "Sustainability",
    "text": "Sustainability\nID: sustainability\nSee also: stability; feasability; N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The ability of the surveillance system to be ongoing in the long term.\nDescription: Sustainability refers to the system’s ability to operate effectively over time with consistent support. This includes having skilled personnel, sufficient laboratory capacity for timely sample processing, and clearly defined responsibilities for resource provision. Resources should align with current operational needs. Sustainability is closely linked to usefulness and timeliness, emphasizing practical procedures, adequate budgeting, and long-term system maintenance. It is dependednt on the continued engagement from stakeholders/actors involved in core surveillance functions, as well as the acceptability of end-users of the surveillance system.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#system-description",
    "href": "dictionary_documentation.html#system-description",
    "title": "dictionary documentation",
    "section": "System description",
    "text": "System description\nID: systemDescription\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: TBD\nDescription: N/A\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#technical-competence-and-training",
    "href": "dictionary_documentation.html#technical-competence-and-training",
    "title": "dictionary documentation",
    "section": "Technical competence and training",
    "text": "Technical competence and training\nID: technicalCompetenceAndTraining\nSee also: laboratory and sample management\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: Refers to the technical skills of the personnel involved in the surveillance system, including access to relevant training.\nDescription: The combination of the technical skills, knowledge, and capacities required by personnel to effectively implement and sustain a surveillance system, supported by access to appropriate initial and ongoing training programs. It encompasses the ability to perform system functions accurately, ensure data quality, and engage in collaborative activities, thereby enhancing the overall performance and reliability of the surveillance system\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#timeliness",
    "href": "dictionary_documentation.html#timeliness",
    "title": "dictionary documentation",
    "section": "Timeliness",
    "text": "Timeliness\nID: timeliness\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The time between any two steps in the surveillance process.\nDescription: Timeliness refers to the speed at which data are collected, reported, analyzed, and used to guide public health action. The required level of timeliness depends on the health event—rapid reporting is critical for acute, highly transmissible diseases, while less urgent conditions may allow for slower reporting. Timeliness is influenced by surveillance system design, resource availability, and event complexity. Delays in lab results or data flow can hinder timely response, especially in resource-limited settings. It must be balanced with accuracy and completeness and aligned with surveillance goals such as outbreak detection, trend monitoring, or long-term planning.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#traceability-data",
    "href": "dictionary_documentation.html#traceability-data",
    "title": "dictionary documentation",
    "section": "Traceability (data)",
    "text": "Traceability (data)\nID: traceabilityData\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: N/A\nDescription: TBD\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#transparency",
    "href": "dictionary_documentation.html#transparency",
    "title": "dictionary documentation",
    "section": "Transparency",
    "text": "Transparency\nID: transparency\nSee also: N/A\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: The extent to which information can be and is shared across member agencies.\nDescription: Transparency refers to how openly the surveillance system shares information about its data collection methods, analysis processes, and limitations with stakeholders and end-users. High transparency builds trust, supports data interpretation, and facilitates informed decision-making. Challenges include inconsistent or delayed data sharing, which can reduce stakeholder confidence and limit timely responses. Transparency is linked to terms like acceptability and usefulness, as clear communication of methods and constraints helps users understand system strengths and weaknesses.\nBackground: TBD\nMeasurement: Measurement may involve assessing frequency, clarity, and accessibility of data reports and documentation."
  },
  {
    "objectID": "dictionary_documentation.html#usablity",
    "href": "dictionary_documentation.html#usablity",
    "title": "dictionary documentation",
    "section": "Usablity",
    "text": "Usablity\nID: usability\nSee also: usefulness; communication dissemination\nSynonyms\n\nExact: N/A\nBroad: N/A\n\nDefinition: Data and products produced by the surveillance system should be tailored to meet the needs of intended users.\nDescription: Usability assesses whether surveillance data and products are tailored to meet the needs of intended users. High usability involves clear documentation (data dictionaries, manuals), minimal training requirements, and system workflows that align with user practices. It also considers how well data interpretations (e.g., statistics) are communicated, including accessibility features like language, cultural relevance, and format. The ability to stratify data by relevant factors (age, sex, geography) enhances usefulness. Regular user feedback guides system improvements. Usability links closely with acceptability and usefulness, influencing how effectively data informs public health actions.\nBackground: TBD\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#usefulness",
    "href": "dictionary_documentation.html#usefulness",
    "title": "dictionary documentation",
    "section": "Usefulness",
    "text": "Usefulness\nID: usefulness\nSee also: effectiveness; benefit\nSynonyms\n\nExact: impact\nBroad: N/A\n\nDefinition: The extent to which the surveillance system contributes to prevention and control of health-related events by providing practical value that supports public health actions and policy decisions (4,anonymous.2001?,Azofeifa_EBHS_2018?,WorldBank_PPH_2006?).\nDescription: Usefulness encompasses two complementary facets: the system’s contribution to preventing or controlling health events (the direct public health outcome) and the practical value of surveillance outputs for decision-makers (the mechanism through which impact occurs). It is demonstrated through real-world outcomes such as policy changes, early outbreak detection, targeted interventions, or improved understanding of health threats that guide evidence-based responses (Groseclose_APHS_2017?,Peyre_TRVA_2019?). A surveillance system is considered useful when it provides value to stakeholders and society by enabling meaningful actions that would not otherwise occur (4,DelRioVilas_HSEitPC_2022?). The assessment of usefulness should begin with a review of system objectives and involves measuring how well surveillance outputs meet stakeholder needs and influence decision-making (anonymous.2001?,Azofeifa_EBHS_2018?). Both qualitative approaches (subjective views of users) and quantitative approaches (measuring impact on policies, interventions, or disease occurrence) can assess usefulness, though measurement is often considered inexact (Thacker_MEES_1988?,Declich_PHS_1994?). Retrospective analysis and stakeholder input help assess whether the system aligns with health priorities and produces information that stakeholders find valuable and actionable. Usefulness is closely tied to—and affected by—virtually all other surveillance attributes including sensitivity, timeliness, predictive value positive, representativeness, simplicity, flexibility, acceptability, and stability. For example, improved timeliness allows control and prevention activities to be initiated earlier, directly enhancing overall usefulness (anonymous.2001?). The quality of data determined by these underlying attributes ultimately shapes the system’s value in guiding effective, evidence-based responses and demonstrating that surveillance efforts justify their investment.\nBackground: Usefulness represents one of the seven original core attributes established for evaluating public health surveillance systems, with foundational definition and emphasis provided by Thacker et al. beginning in 1988 and subsequently codified in CDC evaluation guidelines (Thacker_MEES_1988?,anonymous.2001?,Groseclose_EPHS_2010?). This provenance establishes usefulness as a cornerstone concept in public health surveillance evaluation, reflecting the fundamental question of whether surveillance systems achieve their intended purpose of improving population health. The attribute has been consistently included in evaluations of diverse surveillance contexts including behavioral health systems, wastewater surveillance, and global public health surveillance under International Health Regulations (IHR 2005) (Azofeifa_EBHS_2018?,Amato_E_2023?). Usefulness encompasses two distinct but complementary facets identified in surveillance evaluation literature. The first represents the direct purpose of surveillance—the public health outcome itself, measured by the system’s contribution to disease reduction, control, or improved understanding (anonymous.2001?,Thacker_MEES_1988?). The second represents the mechanism of impact—how surveillance data translates into policy and action, reflecting the system’s demonstrable utility for stakeholders and decision-makers (WorldBank_PPH_2006?,DelRioVilas_HSEitPC_2022?). Both facets are essential: a system must both achieve public health outcomes and provide practical value to end-users to be considered truly useful. This dual nature is reflected in assessment approaches that combine both qualitative measures (subjective views of users regarding practical value) and quantitative measures (documented impacts on policies, interventions, or disease occurrence) (Thacker_MEES_1988?,Declich_PHS_1994?). Usefulness is explicitly recognized as synonymous with impact in evaluation literature, yet the terms reflect distinct emphasis and measurement traditions (dictionary_documentation?). While usefulness is typically framed around contributions to prevention and control or informing public health policy decisions, impact is often defined more specifically as the changes made based on surveillance results, frequently appearing in animal health, economic evaluation, and One Health frameworks (anonymous.2001?,Azofeifa_EBHS_2018?,Drewe_SERVAL_2015?,Meynard_PFAFM_2008?). The term \"usefulness\" emphasizes the practical, stakeholder-centered perspective of public health practice—whether the system provides value to those who use it and leads to meaningful action. This contrasts subtly with \"impact,\" which tends to emphasize measurable outcomes and return on investment, particularly in contexts requiring economic evaluation or demonstrating the value of integrated surveillance across sectors. The conceptual alignment between usefulness and impact is substantial: both measure the ultimate value of surveillance by assessing tangible benefits, both focus on how surveillance influences decisions and interventions, both depend on the same underlying technical attributes, and both are highly relevant to stakeholders and decision-makers who need evidence that systems justify investment (4,Groseclose_APHS_2017?,Peyre_TRVA_2019?,DelRioVilas_HSEitPC_2022?). The choice between terms often reflects evaluation context and audience rather than fundamental conceptual difference—public health surveillance evaluations rooted in CDC traditions typically emphasize usefulness, while One Health and economic evaluations frequently emphasize impact. Regardless of terminology, the attribute represents a summary measure of whether the surveillance system achieves its ultimate purpose: improving health outcomes through informed action. Modern surveillance evaluation increasingly demands rigorous assessment of this attribute for both donor accountability and social accountability, requiring demonstration that surveillance efforts produce measurable value (4,Peyre_AITAEoPHS_2022?).\nMeasurement: TBD"
  },
  {
    "objectID": "dictionary_documentation.html#markdown-rendering-issues",
    "href": "dictionary_documentation.html#markdown-rendering-issues",
    "title": "dictionary documentation",
    "section": "Markdown Rendering Issues",
    "text": "Markdown Rendering Issues\nThe following fields had issues rendering markdown:\n\ndataQuality (term_background): Unclosed change tracking markup found"
  }
]